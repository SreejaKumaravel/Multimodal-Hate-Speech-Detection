{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qq2-qzezUZhB",
        "outputId": "a716e71f-0d6e-4edb-efc4-181ae82d5f1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text"
      ],
      "metadata": {
        "id": "RJAfk5vorVXd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Logistic** **Regression**"
      ],
      "metadata": {
        "id": "Fa4QHpytYj9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Tamil Stopwords (expand as necessary)\n",
        "tamil_stopwords = set([\n",
        "    \"அது\", \"இந்த\", \"இது\", \"உங்கள்\", \"அவள்\", \"ஆனால்\", \"என்ற\", \"இருந்து\", \"தான்\", \"எழுதி\",\n",
        "    \"ஒரு\", \"உள்ள\", \"என்றும்\", \"உடன்\", \"அந்த\", \"என்பது\", \"இயற்கை\", \"வாங்க\", \"அல்லது\", \"இடையே\"\n",
        "])\n",
        "\n",
        "# Load the Tamil dataset (adjust file paths as needed)\n",
        "train_data = pd.read_csv(\"/content/drive/MyDrive/Project/Tamil/Text/TA-AT-train.csv\")  # Training data with text and labels\n",
        "test_data = pd.read_csv(\"/content/drive/MyDrive/Project/Test/Tamil/TA-AT-test.csv\")  # Test data with text only\n",
        "\n",
        "# Step 1: Preprocess Tamil text\n",
        "def preprocess_text_tamil(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation, digits, and special characters, keeping only Tamil alphabets and spaces\n",
        "    text = ''.join(char for char in text if '\\u0B80' <= char <= '\\u0BFF' or char.isspace())\n",
        "    # Remove stopwords\n",
        "    text = ' '.join([word for word in text.split() if word not in tamil_stopwords])\n",
        "    return text\n",
        "\n",
        "train_data['Transcript'] = train_data['Transcript'].apply(preprocess_text_tamil)\n",
        "test_data['Transcript'] = test_data['Transcript'].apply(preprocess_text_tamil)\n",
        "\n",
        "# Step 2: Check class distribution in training data\n",
        "class_counts = train_data['Class Label Short'].value_counts()\n",
        "print(\"Class distribution:\", class_counts)\n",
        "\n",
        "# Step 3: Compute class weights for imbalanced dataset\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(train_data['Class Label Short']), y=train_data['Class Label Short'])\n",
        "class_weight_dict = dict(zip(np.unique(train_data['Class Label Short']), class_weights))\n",
        "print(\"Class weights:\", class_weight_dict)\n",
        "\n",
        "# Step 4: Prepare the text data (transcripts) and labels\n",
        "X_train = train_data['Transcript']\n",
        "y_train = train_data['Class Label Short']\n",
        "X_test = test_data['Transcript']\n",
        "\n",
        "# Convert labels to numeric format\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_numeric = label_encoder.fit_transform(y_train)\n",
        "\n",
        "# Step 5: Extract features using CountVectorizer\n",
        "vectorizer = CountVectorizer(max_features=5000, ngram_range=(1, 2), analyzer='char', max_df=0.9, min_df=5)\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Step 6: Choose and train a machine learning model\n",
        "# Logistic Regression\n",
        "model = LogisticRegression(class_weight='balanced', solver='liblinear', random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_vec, y_train_numeric)\n",
        "\n",
        "# Step 7: Predict class labels for test data\n",
        "y_test_pred = model.predict(X_test_vec)\n",
        "\n",
        "# Convert numeric predictions back to string labels\n",
        "y_test_pred_labels = label_encoder.inverse_transform(y_test_pred)\n",
        "\n",
        "# Step 8: Save predictions in a TSV file with 'filename' and 'Predicted_Label' columns\n",
        "test_data['Predicted_Label'] = y_test_pred_labels\n",
        "\n",
        "# Assuming 'File Name' is a column in test_data (adjust if needed)\n",
        "predictions = test_data[['File Name', 'Predicted_Label']]\n",
        "\n",
        "# Save to TSV file\n",
        "predictions.to_csv(\"CV_Logistic_prediction.tsv\", sep='\\t', index=False)\n",
        "print(\"Predictions saved to CountVectorizer_Logistic_prediction.tsv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArJHoIAAXKg1",
        "outputId": "d219b3fb-8723-4e14-e3d0-3ce8a043f556"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution: Class Label Short\n",
            "N    287\n",
            "G     68\n",
            "C     65\n",
            "R     61\n",
            "P     33\n",
            "Name: count, dtype: int64\n",
            "Class weights: {'C': 1.5815384615384616, 'G': 1.511764705882353, 'N': 0.3581881533101045, 'P': 3.1151515151515152, 'R': 1.6852459016393442}\n",
            "Predictions saved to CountVectorizer_Logistic_prediction.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SVM**"
      ],
      "metadata": {
        "id": "e3l_N_QLYq6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Tamil Stopwords (expand as necessary)\n",
        "tamil_stopwords = set([\n",
        "    \"அது\", \"இந்த\", \"இது\", \"உங்கள்\", \"அவள்\", \"ஆனால்\", \"என்ற\", \"இருந்து\", \"தான்\", \"எழுதி\",\n",
        "    \"ஒரு\", \"உள்ள\", \"என்றும்\", \"உடன்\", \"அந்த\", \"என்பது\", \"இயற்கை\", \"வாங்க\", \"அல்லது\", \"இடையே\"\n",
        "])\n",
        "\n",
        "# Load the Tamil dataset (adjust file paths as needed)\n",
        "train_data = pd.read_csv(\"/content/drive/MyDrive/Project/Tamil/Text/TA-AT-train.csv\")  # Training data with text and labels\n",
        "test_data = pd.read_csv(\"/content/drive/MyDrive/Project/Test/Tamil/TA-AT-test.csv\")  # Test data with text only\n",
        "\n",
        "# Step 1: Preprocess Tamil text\n",
        "def preprocess_text_tamil(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation, digits, and special characters, keeping only Tamil alphabets and spaces\n",
        "    text = ''.join(char for char in text if '\\u0B80' <= char <= '\\u0BFF' or char.isspace())\n",
        "    # Remove stopwords\n",
        "    text = ' '.join([word for word in text.split() if word not in tamil_stopwords])\n",
        "    return text\n",
        "\n",
        "train_data['Transcript'] = train_data['Transcript'].apply(preprocess_text_tamil)\n",
        "test_data['Transcript'] = test_data['Transcript'].apply(preprocess_text_tamil)\n",
        "\n",
        "# Step 2: Check class distribution in training data\n",
        "class_counts = train_data['Class Label Short'].value_counts()\n",
        "print(\"Class distribution:\", class_counts)\n",
        "\n",
        "# Step 3: Compute class weights for imbalanced dataset\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(train_data['Class Label Short']), y=train_data['Class Label Short'])\n",
        "class_weight_dict = dict(zip(np.unique(train_data['Class Label Short']), class_weights))\n",
        "print(\"Class weights:\", class_weight_dict)\n",
        "\n",
        "# Step 4: Prepare the text data (transcripts) and labels\n",
        "X_train = train_data['Transcript']\n",
        "y_train = train_data['Class Label Short']\n",
        "X_test = test_data['Transcript']\n",
        "\n",
        "# Convert labels to numeric format\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_numeric = label_encoder.fit_transform(y_train)\n",
        "\n",
        "# Step 5: Extract TF-IDF features from Tamil text\n",
        "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), analyzer='char', max_df=0.9, min_df=5)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Step 6: Choose and train a machine learning model\n",
        "# Support Vector Machine\n",
        "model = SVC(kernel='sigmoid', class_weight='balanced', probability=True, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_tfidf, y_train_numeric)\n",
        "\n",
        "# Step 7: Predict class labels for test data\n",
        "y_test_pred = model.predict(X_test_tfidf)\n",
        "\n",
        "# Convert numeric predictions back to string labels\n",
        "y_test_pred_labels = label_encoder.inverse_transform(y_test_pred)\n",
        "\n",
        "# Step 8: Save predictions in a TSV file with 'filename' and 'Predicted_Label' columns\n",
        "test_data['Predicted_Label'] = y_test_pred_labels\n",
        "\n",
        "# Assuming 'File Name' is a column in test_data (adjust if needed)\n",
        "predictions = test_data[['File Name', 'Predicted_Label']]\n",
        "\n",
        "# Save to TSV file\n",
        "predictions.to_csv(\"SVM_prediction.tsv\", sep='\\t', index=False)\n",
        "print(\"Predictions saved to SVM_prediction.tsv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYeQ5rkpYGgu",
        "outputId": "011a8743-6036-482f-f820-93f0be341bc4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution: Class Label Short\n",
            "N    287\n",
            "G     68\n",
            "C     65\n",
            "R     61\n",
            "P     33\n",
            "Name: count, dtype: int64\n",
            "Class weights: {'C': 1.5815384615384616, 'G': 1.511764705882353, 'N': 0.3581881533101045, 'P': 3.1151515151515152, 'R': 1.6852459016393442}\n",
            "Predictions saved to SVM_prediction.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MLP Classifier**"
      ],
      "metadata": {
        "id": "wT7jh2X7Z2GF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Tamil Stopwords (example - expand this list as necessary)\n",
        "tamil_stopwords = set([\n",
        "    \"அது\", \"இந்த\", \"இது\", \"உங்கள்\", \"அவள்\", \"ஆனால்\", \"என்ற\", \"இருந்து\", \"தான்\", \"எழுதி\",\n",
        "    \"ஒரு\", \"உள்ள\", \"என்றும்\", \"உடன்\", \"அந்த\", \"என்பது\", \"இயற்கை\", \"வாங்க\", \"அல்லது\", \"இடையே\"\n",
        "])\n",
        "\n",
        "# Load the Tamil dataset (adjust the file paths as needed)\n",
        "train_data = pd.read_csv(\"/content/drive/MyDrive/Project/Tamil/Text/TA-AT-train.csv\")  # Training data with text and labels\n",
        "test_data = pd.read_csv(\"/content/drive/MyDrive/Project/Test/Tamil/TA-AT-test.csv\")  # Test data with text only\n",
        "\n",
        "# Step 1: Preprocess Tamil text\n",
        "def preprocess_text_tamil(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation, digits, and special characters, keeping only Tamil alphabets and spaces\n",
        "    text = ''.join(char for char in text if '\\u0B80' <= char <= '\\u0BFF' or char.isspace())\n",
        "    # Remove stopwords\n",
        "    text = ' '.join([word for word in text.split() if word not in tamil_stopwords])\n",
        "    return text\n",
        "\n",
        "train_data['Transcript'] = train_data['Transcript'].apply(preprocess_text_tamil)\n",
        "test_data['Transcript'] = test_data['Transcript'].apply(preprocess_text_tamil)\n",
        "\n",
        "# Step 2: Check class distribution in training data\n",
        "class_counts = train_data['Class Label Short'].value_counts()\n",
        "print(\"Class distribution:\", class_counts)\n",
        "\n",
        "# Step 3: Compute class weights for imbalanced dataset\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(train_data['Class Label Short']), y=train_data['Class Label Short'])\n",
        "class_weight_dict = dict(zip(np.unique(train_data['Class Label Short']), class_weights))\n",
        "print(\"Class weights:\", class_weight_dict)\n",
        "\n",
        "# Step 4: Prepare the text data (transcripts) and labels\n",
        "X_train = train_data['Transcript']\n",
        "y_train = train_data['Class Label Short']\n",
        "X_test = test_data['Transcript']\n",
        "\n",
        "# Convert labels to numeric format\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_numeric = label_encoder.fit_transform(y_train)\n",
        "\n",
        "# Step 5: Extract TF-IDF features from Tamil text\n",
        "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), analyzer='char', max_df=0.9, min_df=5)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Step 6: Train an MLPClassifier\n",
        "mlp_model = MLPClassifier(hidden_layer_sizes=(128, 64), activation='relu', solver='adam', max_iter=50, random_state=42)\n",
        "mlp_model.fit(X_train_tfidf, y_train_numeric)\n",
        "\n",
        "# Step 7: Predict class labels for test data\n",
        "y_test_pred = mlp_model.predict(X_test_tfidf)\n",
        "\n",
        "# Convert numeric predictions back to string labels\n",
        "y_test_pred_labels = label_encoder.inverse_transform(y_test_pred)\n",
        "\n",
        "# Step 8: Save predictions in a TSV file with 'filename' and 'Predicted_Label' columns\n",
        "test_data['Predicted_Label'] = y_test_pred_labels\n",
        "\n",
        "# Assuming 'File Name' is a column in test_data (adjust if needed)\n",
        "predictions = test_data[['File Name', 'Predicted_Label']]\n",
        "\n",
        "# Save to TSV file\n",
        "predictions.to_csv(\"MLP_Prediction.tsv\", sep='\\t', index=False)\n",
        "print(\"Predictions saved to MLP_Prediction.tsv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2XaNnXBvZB5n",
        "outputId": "6757d54b-9e48-4317-9d38-898a41d46e04"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution: Class Label Short\n",
            "N    287\n",
            "G     68\n",
            "C     65\n",
            "R     61\n",
            "P     33\n",
            "Name: count, dtype: int64\n",
            "Class weights: {'C': 1.5815384615384616, 'G': 1.511764705882353, 'N': 0.3581881533101045, 'P': 3.1151515151515152, 'R': 1.6852459016393442}\n",
            "Predictions saved to MLP_Prediction.tsv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the TSV file\n",
        "file_path = \"/content/SVM_prediction.tsv\"\n",
        "df = pd.read_csv(file_path, sep=\"\\t\")\n",
        "\n",
        "# Change column name (e.g., from 'Old_Column_Name' to 'New_Column_Name')\n",
        "df.rename(columns={\"File Name\": \"File_Name\"}, inplace=True)\n",
        "\n",
        "# Save the updated DataFrame back to the TSV file\n",
        "output_path = \"/content/SVM_prediction.tsv\"\n",
        "df.to_csv(output_path, sep=\"\\t\", index=False)\n",
        "\n",
        "print(f\"Updated file saved to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKvyEtg8bOgd",
        "outputId": "bd64aa9c-7469-493d-9995-6aab368c4c73"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated file saved to /content/SVM_prediction.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fusion with weight**"
      ],
      "metadata": {
        "id": "SGLXbJVMpTwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# File paths (update if necessary)\n",
        "logistic_predictions_path = \"/content/CV_Logistic_prediction.tsv\"\n",
        "mlp_predictions_path = \"/content/MLP_Prediction.tsv\"\n",
        "svm_predictions_path = \"/content/SVM_prediction.tsv\"  # New file\n",
        "output_fused_predictions_path =\"/content/Text_fusion_weight.tsv\"\n",
        "\n",
        "\n",
        "# Load predictions\n",
        "logistic_predictions = pd.read_csv(logistic_predictions_path, sep=\"\\t\")\n",
        "mlp_predictions = pd.read_csv(mlp_predictions_path, sep=\"\\t\")\n",
        "svm_predictions = pd.read_csv(svm_predictions_path, sep=\"\\t\")  # Load additional predictions\n",
        "\n",
        "# Ensure all files have the same filenames in the same order\n",
        "if not (\n",
        "    all(logistic_predictions['File_Name'] == mlp_predictions['File_Name']) and\n",
        "    all(logistic_predictions['File_Name'] == svm_predictions['File_Name'])\n",
        "):\n",
        "    raise ValueError(\"Mismatch in filenames between the prediction files.\")\n",
        "\n",
        "# Majority fusion with different weights\n",
        "fused_predictions = []\n",
        "for _, (log_row, mlp_row, svm_row) in enumerate(zip(\n",
        "    logistic_predictions.itertuples(),\n",
        "    mlp_predictions.itertuples(),\n",
        "    svm_predictions.itertuples()\n",
        ")):\n",
        "    File_Name = log_row.File_Name\n",
        "    log_label = log_row.Predicted_Label\n",
        "    mlp_label = mlp_row.Predicted_Label\n",
        "    svm_label = svm_row.Predicted_Label\n",
        "\n",
        "    # Weighted voting\n",
        "    vote_counter = Counter()\n",
        "    vote_counter[log_label] += 1 # Weight 1 for Logistic regression\n",
        "    vote_counter[mlp_label] +=  3 # Weight 2 for MLP\n",
        "    vote_counter[svm_label] += 2  # Weight 1 for additional model\n",
        "\n",
        "    # Majority vote\n",
        "    fused_label = vote_counter.most_common(1)[0][0]\n",
        "    fused_predictions.append({\"File_Name\": File_Name, \"fused_label\": fused_label})\n",
        "\n",
        "# Save fused predictions to a new TSV file\n",
        "fused_predictions_df = pd.DataFrame(fused_predictions)\n",
        "fused_predictions_df.to_csv(output_fused_predictions_path, sep=\"\\t\", index=False)\n",
        "print(f\"Fused predictions saved to {output_fused_predictions_path}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFeD9uyjZ0xG",
        "outputId": "ee133233-638f-4c41-adb5-6d724ba0491d"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fused predictions saved to /content/Text_fusion_weight.tsv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fusion without weight**"
      ],
      "metadata": {
        "id": "RJMyoWbEpYXM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# File paths (update if necessary)\n",
        "logistic_predictions_path = \"/content/CV_Logistic_prediction.tsv\"\n",
        "mlp_predictions_path = \"/content/MLP_Prediction.tsv\"\n",
        "svm_predictions_path = \"/content/SVM_prediction.tsv\"  # New file\n",
        "output_fused_predictions_path = \"/content/Text_fusion.tsv\"\n",
        "\n",
        "# Load predictions\n",
        "logistic_predictions = pd.read_csv(logistic_predictions_path, sep=\"\\t\")\n",
        "mlp_predictions = pd.read_csv(mlp_predictions_path, sep=\"\\t\")\n",
        "svm_predictions = pd.read_csv(svm_predictions_path, sep=\"\\t\")  # Load additional predictions\n",
        "\n",
        "# Ensure all files have the same filenames in the same order\n",
        "if not (\n",
        "    all(logistic_predictions['File_Name'] == mlp_predictions['File_Name']) and\n",
        "    all(logistic_predictions['File_Name'] == svm_predictions['File_Name'])\n",
        "):\n",
        "    raise ValueError(\"Mismatch in filenames between the prediction files.\")\n",
        "\n",
        "# Majority fusion without weighting\n",
        "fused_predictions = []\n",
        "for _, (log_row, mlp_row, svm_row) in enumerate(zip(\n",
        "    logistic_predictions.itertuples(),\n",
        "    mlp_predictions.itertuples(),\n",
        "    svm_predictions.itertuples()\n",
        ")):\n",
        "    File_Name = log_row.File_Name\n",
        "    log_label = log_row.Predicted_Label\n",
        "    mlp_label = mlp_row.Predicted_Label\n",
        "    svm_label = svm_row.Predicted_Label\n",
        "\n",
        "    # Equal voting\n",
        "    vote_counter = Counter([log_label, mlp_label, svm_label])\n",
        "\n",
        "    # Majority vote\n",
        "    fused_label = vote_counter.most_common(1)[0][0]\n",
        "    fused_predictions.append({\"File_Name\": File_Name, \"fused_label\": fused_label})\n",
        "\n",
        "# Save fused predictions to a new TSV file\n",
        "fused_predictions_df = pd.DataFrame(fused_predictions)\n",
        "fused_predictions_df.to_csv(output_fused_predictions_path, sep=\"\\t\", index=False)\n",
        "print(f\"Fused predictions saved to {output_fused_predictions_path}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIX1hvr4dhDn",
        "outputId": "3e307ca4-ecda-400b-8732-6ceddf1a7d74"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fused predictions saved to /content/Text_fusion.tsv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Audio"
      ],
      "metadata": {
        "id": "XXUyPV-adtoT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MLP Classifierr**"
      ],
      "metadata": {
        "id": "F47q1XoxdxTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import librosa\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Paths (update with actual paths)\n",
        "train_csv_path = \"/content/drive/MyDrive/Project/Tamil/Text/TA-AT-train.csv\"\n",
        "train_audio_dir = \"/content/drive/MyDrive/Project/Tamil/audio\"\n",
        "test_audio_dir = \"/content/drive/MyDrive/Project/Test/Tamil/audio\"\n",
        "output_tsv_path = \"MLP_audio_predictions.tsv\"\n",
        "\n",
        "# Step 1: Load train.csv to map class labels\n",
        "train_df = pd.read_csv(train_csv_path)\n",
        "train_df['Class Label Short'] = train_df['Class Label Short'].astype(str)\n",
        "\n",
        "# Step 2: Preprocessing function for audio data\n",
        "def preprocess_audio(file_path, sr=16000, n_mfcc=13):\n",
        "    try:\n",
        "        # Load the audio file\n",
        "        y, original_sr = librosa.load(file_path, sr=None)\n",
        "        # Resample the audio if needed\n",
        "        if original_sr != sr:\n",
        "            y = librosa.resample(y, orig_sr=original_sr, target_sr=sr)\n",
        "        # Trim silence from the beginning and end\n",
        "        y, _ = librosa.effects.trim(y)\n",
        "        # Normalize amplitude\n",
        "        y = librosa.util.normalize(y)\n",
        "        # Extract MFCCs\n",
        "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
        "        # Compute mean across time frames for fixed-length vector\n",
        "        mfcc_mean = np.mean(mfccs, axis=1)\n",
        "        return mfcc_mean\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Step 3: Extract features and labels for training\n",
        "X_train, y_train = [], []\n",
        "for _, row in train_df.iterrows():\n",
        "    file_path = os.path.join(train_audio_dir, row['File Name'] + \".wav\")  # Add .wav extension\n",
        "    if os.path.exists(file_path):\n",
        "        features = preprocess_audio(file_path)\n",
        "        if features is not None:\n",
        "            X_train.append(features)\n",
        "            y_train.append(row['Class Label Short'])\n",
        "        else:\n",
        "            print(f\"Skipping file {file_path} due to feature extraction failure.\")\n",
        "    else:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "\n",
        "# Check if X_train is empty\n",
        "if X_train.size == 0:\n",
        "    print(\"No valid training data found. Please check file paths and data.\")\n",
        "else:\n",
        "    # Proceed with model training\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "\n",
        "    # Step 4: Compute class weights\n",
        "    class_weights = compute_class_weight(\n",
        "        class_weight=\"balanced\",\n",
        "        classes=np.unique(y_train_encoded),\n",
        "        y=y_train_encoded\n",
        "    )\n",
        "    class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "    print(f\"Class weights: {class_weight_dict}\")\n",
        "\n",
        "    # Step 5: Train MLPClassifier\n",
        "    clf = MLPClassifier(\n",
        "        hidden_layer_sizes=(128, 64),\n",
        "        activation='relu',\n",
        "        solver='adam',\n",
        "        max_iter=200,\n",
        "        random_state=42\n",
        "    )\n",
        "    clf.fit(X_train, y_train_encoded)\n",
        "    print(\"MLPClassifier trained successfully.\")\n",
        "\n",
        "# Step 6: Predict for test data\n",
        "test_predictions = []\n",
        "test_files = [f for f in os.listdir(test_audio_dir) if f.endswith('.wav')]\n",
        "for file_name in test_files:\n",
        "    file_path = os.path.join(test_audio_dir, file_name)\n",
        "    features = preprocess_audio(file_path)\n",
        "    if features is not None:\n",
        "        predicted_label_encoded = clf.predict([features])\n",
        "        predicted_label = label_encoder.inverse_transform(predicted_label_encoded)[0]\n",
        "        test_predictions.append({\"File_Name\": file_name, \"predicted_label\": predicted_label})\n",
        "    else:\n",
        "        print(f\"Skipping file {file_path} due to feature extraction failure.\")\n",
        "\n",
        "# Step 7: Save predictions to a TSV file\n",
        "test_predictions_df = pd.DataFrame(test_predictions)\n",
        "test_predictions_df.to_csv(output_tsv_path, sep=\"\\t\", index=False)\n",
        "print(f\"Predictions saved to {output_tsv_path}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwP87vYnd07u",
        "outputId": "41db3095-e70d-45cc-9bd9-8f286495f1c9"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File not found: /content/drive/MyDrive/Project/Tamil/audio/H_TA_002_G_M_037_001.wav\n",
            "File not found: /content/drive/MyDrive/Project/Tamil/audio/H_TA_002_G_M_037_002.wav\n",
            "File not found: /content/drive/MyDrive/Project/Tamil/audio/H_TA_002_G_M_038_001.wav\n",
            "File not found: /content/drive/MyDrive/Project/Tamil/audio/H_TA_002_G_M_038_002.wav\n",
            "File not found: /content/drive/MyDrive/Project/Tamil/audio/H_TA_002_G_M_038_003.wav\n",
            "Class weights: {0: 1.5661538461538462, 1: 1.615873015873016, 2: 0.35470383275261325, 3: 3.084848484848485, 4: 1.6688524590163933}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLPClassifier trained successfully.\n",
            "Predictions saved to MLP_audio_predictions.tsv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SVM**"
      ],
      "metadata": {
        "id": "_x84ppxXfxd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import librosa\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Paths (update with actual paths)\n",
        "train_csv_path = \"/content/drive/MyDrive/Project/Tamil/Text/TA-AT-train.csv\"\n",
        "train_audio_dir = \"/content/drive/MyDrive/Project/Tamil/audio\"\n",
        "test_audio_dir = \"/content/drive/MyDrive/Project/Test/Tamil/audio\"\n",
        "output_tsv_path = \"SVM_audio_predictions.tsv\"\n",
        "\n",
        "# Step 1: Load train.csv to map class labels\n",
        "train_df = pd.read_csv(train_csv_path)\n",
        "train_df['Class Label Short'] = train_df['Class Label Short'].astype(str)\n",
        "\n",
        "# Step 2: Preprocessing function for audio data\n",
        "def preprocess_audio(file_path, sr=16000, n_mfcc=13):\n",
        "    try:\n",
        "        # Load the audio file\n",
        "        y, original_sr = librosa.load(file_path, sr=None)\n",
        "        if original_sr != sr:\n",
        "            y = librosa.resample(y, orig_sr=original_sr, target_sr=sr)\n",
        "        y, _ = librosa.effects.trim(y)\n",
        "        y = librosa.util.normalize(y)\n",
        "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
        "        return np.mean(mfccs, axis=1)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Step 3: Extract features and labels for training\n",
        "X_train, y_train = [], []\n",
        "for _, row in train_df.iterrows():\n",
        "    file_path = os.path.join(train_audio_dir, row['File Name'] + \".wav\")\n",
        "    if os.path.exists(file_path):\n",
        "        features = preprocess_audio(file_path)\n",
        "        if features is not None:\n",
        "            X_train.append(features)\n",
        "            y_train.append(row['Class Label Short'])\n",
        "        else:\n",
        "            print(f\"Skipping file {file_path} due to feature extraction failure.\")\n",
        "    else:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "\n",
        "# Check if X_train is empty\n",
        "if X_train.size == 0:\n",
        "    print(\"No valid training data found. Please check file paths and data.\")\n",
        "else:\n",
        "    # Proceed with model training\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "\n",
        "    # Step 4: Compute class weights\n",
        "    class_weights = compute_class_weight(\n",
        "        class_weight=\"balanced\",\n",
        "        classes=np.unique(y_train_encoded),\n",
        "        y=y_train_encoded\n",
        "    )\n",
        "    class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "    print(f\"Class weights: {class_weight_dict}\")\n",
        "\n",
        "    # Step 5: Standardize features\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "\n",
        "    # Train SVM Classifier\n",
        "    clf = SVC(\n",
        "        kernel='rbf',  # Radial Basis Function kernel\n",
        "        C=1.0,         # Regularization parameter\n",
        "        class_weight=class_weight_dict,\n",
        "        probability=True,  # Enable probability estimates\n",
        "        random_state=42\n",
        "    )\n",
        "    clf.fit(X_train, y_train_encoded)\n",
        "    print(\"SVM classifier trained successfully.\")\n",
        "\n",
        "# Step 6: Predict for test data\n",
        "test_predictions = []\n",
        "test_files = [f for f in os.listdir(test_audio_dir) if f.endswith('.wav')]\n",
        "for file_name in test_files:\n",
        "    file_path = os.path.join(test_audio_dir, file_name)\n",
        "    features = preprocess_audio(file_path)\n",
        "    if features is not None:\n",
        "        features = scaler.transform([features])  # Standardize test features\n",
        "        predicted_label_encoded = clf.predict(features)\n",
        "        predicted_label = label_encoder.inverse_transform(predicted_label_encoded)[0]\n",
        "        test_predictions.append({\"File_Name\": file_name, \"predicted_label\": predicted_label})\n",
        "    else:\n",
        "        print(f\"Skipping file {file_path} due to feature extraction failure.\")\n",
        "\n",
        "# Step 7: Save predictions to a TSV file\n",
        "test_predictions_df = pd.DataFrame(test_predictions)\n",
        "test_predictions_df.to_csv(output_tsv_path, sep=\"\\t\", index=False)\n",
        "print(f\"Predictions saved to {output_tsv_path}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZXe_OvvfwSk",
        "outputId": "06e3b289-f662-4417-ca4f-4c7eabfe38d3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File not found: /content/drive/MyDrive/Project/Tamil/audio/H_TA_002_G_M_037_001.wav\n",
            "File not found: /content/drive/MyDrive/Project/Tamil/audio/H_TA_002_G_M_037_002.wav\n",
            "File not found: /content/drive/MyDrive/Project/Tamil/audio/H_TA_002_G_M_038_001.wav\n",
            "File not found: /content/drive/MyDrive/Project/Tamil/audio/H_TA_002_G_M_038_002.wav\n",
            "File not found: /content/drive/MyDrive/Project/Tamil/audio/H_TA_002_G_M_038_003.wav\n",
            "Class weights: {0: 1.5661538461538462, 1: 1.615873015873016, 2: 0.35470383275261325, 3: 3.084848484848485, 4: 1.6688524590163933}\n",
            "SVM classifier trained successfully.\n",
            "Predictions saved to SVM_audio_predictions.tsv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Forest**"
      ],
      "metadata": {
        "id": "SJhf4Wx-i8aH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import librosa\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Paths (update with actual paths)\n",
        "train_csv_path = \"/content/drive/MyDrive/Project/Tamil/Text/TA-AT-train.csv\"\n",
        "train_audio_dir = \"/content/drive/MyDrive/Project/Tamil/audio\"\n",
        "test_audio_dir = \"/content/drive/MyDrive/Project/Test/Tamil/audio\"\n",
        "output_tsv_path = \"RF_audio_predictions.tsv\"\n",
        "\n",
        "# Step 1: Load train.csv to map class labels\n",
        "train_df = pd.read_csv(train_csv_path)\n",
        "train_df['Class Label Short'] = train_df['Class Label Short'].astype(str)\n",
        "\n",
        "# Step 2: Preprocessing function for audio data\n",
        "def preprocess_audio(file_path, sr=16000, n_mfcc=13):\n",
        "    try:\n",
        "        # Load the audio file\n",
        "        y, original_sr = librosa.load(file_path, sr=None)\n",
        "        if original_sr != sr:\n",
        "            y = librosa.resample(y, orig_sr=original_sr, target_sr=sr)\n",
        "        y, _ = librosa.effects.trim(y)\n",
        "        y = librosa.util.normalize(y)\n",
        "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
        "        return np.mean(mfccs, axis=1)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Step 3: Extract features and labels for training\n",
        "X_train, y_train = [], []\n",
        "for _, row in train_df.iterrows():\n",
        "    file_path = os.path.join(train_audio_dir, row['File Name'] + \".wav\")\n",
        "    if os.path.exists(file_path):\n",
        "        features = preprocess_audio(file_path)\n",
        "        if features is not None:\n",
        "            X_train.append(features)\n",
        "            y_train.append(row['Class Label Short'])\n",
        "        else:\n",
        "            print(f\"Skipping file {file_path} due to feature extraction failure.\")\n",
        "    else:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "\n",
        "# Check if X_train is empty\n",
        "if X_train.size == 0:\n",
        "    print(\"No valid training data found. Please check file paths and data.\")\n",
        "else:\n",
        "    # Proceed with model training\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "\n",
        "    # Step 4: Standardize features\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "\n",
        "    # Train Random Forest Classifier\n",
        "    clf = RandomForestClassifier(\n",
        "        n_estimators=200,  # Number of trees in the forest\n",
        "        max_depth=None,  # No maximum depth (fully grown trees)\n",
        "        min_samples_split=2,  # Minimum samples to split a node\n",
        "        min_samples_leaf=1,  # Minimum samples in a leaf node\n",
        "        random_state=42,\n",
        "        n_jobs=-1  # Use all CPU cores for training\n",
        "    )\n",
        "    clf.fit(X_train, y_train_encoded)\n",
        "    print(\"Random Forest classifier trained successfully.\")\n",
        "\n",
        "# Step 6: Predict for test data\n",
        "test_predictions = []\n",
        "test_files = [f for f in os.listdir(test_audio_dir) if f.endswith('.wav')]\n",
        "for file_name in test_files:\n",
        "    file_path = os.path.join(test_audio_dir, file_name)\n",
        "    features = preprocess_audio(file_path)\n",
        "    if features is not None:\n",
        "        features = scaler.transform([features])  # Standardize test features\n",
        "        predicted_label_encoded = clf.predict(features)\n",
        "        predicted_label = label_encoder.inverse_transform(predicted_label_encoded)[0]\n",
        "        test_predictions.append({\"File_Name\": file_name, \"predicted_label\": predicted_label})\n",
        "    else:\n",
        "        print(f\"Skipping file {file_path} due to feature extraction failure.\")\n",
        "\n",
        "# Step 7: Save predictions to a TSV file\n",
        "test_predictions_df = pd.DataFrame(test_predictions)\n",
        "test_predictions_df.to_csv(output_tsv_path, sep=\"\\t\", index=False)\n",
        "print(f\"Predictions saved to {output_tsv_path}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lccO-hVThA_q",
        "outputId": "15288a7f-d213-4358-c33d-242e33ba153f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File not found: /content/drive/MyDrive/Project/Tamil/audio/H_TA_002_G_M_037_001.wav\n",
            "File not found: /content/drive/MyDrive/Project/Tamil/audio/H_TA_002_G_M_037_002.wav\n",
            "File not found: /content/drive/MyDrive/Project/Tamil/audio/H_TA_002_G_M_038_001.wav\n",
            "File not found: /content/drive/MyDrive/Project/Tamil/audio/H_TA_002_G_M_038_002.wav\n",
            "File not found: /content/drive/MyDrive/Project/Tamil/audio/H_TA_002_G_M_038_003.wav\n",
            "Random Forest classifier trained successfully.\n",
            "Predictions saved to RF_audio_predictions.tsv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the TSV file\n",
        "file_path = \"/content/RF_audio_predictions.tsv\"\n",
        "df = pd.read_csv(file_path, sep=\"\\t\")\n",
        "\n",
        "# Remove the '.wav' extension from the 'filename' column\n",
        "df['File_Name'] = df['File_Name'].str.replace('.wav', '', regex=False)\n",
        "\n",
        "# Save the updated DataFrame back to a new TSV file\n",
        "output_path = \"/content/RF_audio_predictions.tsv\"\n",
        "df.to_csv(output_path, sep=\"\\t\", index=False)\n",
        "\n",
        "print(f\"Updated file saved to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIlocFVMi_8B",
        "outputId": "6265f988-cd7b-475c-c387-d65dc26f9c31"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated file saved to /content/RF_audio_predictions.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fusion with weight**"
      ],
      "metadata": {
        "id": "NQz2DzMhproq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# File paths (update if necessary)\n",
        "svm_predictions_path = \"/content/SVM_audio_predictions.tsv\"\n",
        "mlp_predictions_path = \"/content/MLP_audio_predictions.tsv\"\n",
        "rf_predictions_path = \"/content/RF_audio_predictions.tsv\"  # New file\n",
        "output_fused_predictions_path =\"/content/Audio_fusion_weight.tsv\"\n",
        "\n",
        "\n",
        "# Load predictions\n",
        "svm_predictions = pd.read_csv(svm_predictions_path, sep=\"\\t\")\n",
        "mlp_predictions = pd.read_csv(mlp_predictions_path, sep=\"\\t\")\n",
        "rf_predictions = pd.read_csv(rf_predictions_path, sep=\"\\t\")  # Load additional predictions\n",
        "\n",
        "# Ensure all files have the same filenames in the same order\n",
        "if not (\n",
        "    all(svm_predictions['File_Name'] == mlp_predictions['File_Name']) and\n",
        "    all(svm_predictions['File_Name'] == rf_predictions['File_Name'])\n",
        "):\n",
        "    raise ValueError(\"Mismatch in filenames between the prediction files.\")\n",
        "\n",
        "# Majority fusion with different weights\n",
        "fused_predictions = []\n",
        "for _, (svm_row, mlp_row, rf_row) in enumerate(zip(\n",
        "    svm_predictions.itertuples(),\n",
        "    mlp_predictions.itertuples(),\n",
        "    rf_predictions.itertuples()\n",
        ")):\n",
        "    File_Name = svm_row.File_Name\n",
        "    svm_label = svm_row.predicted_label\n",
        "    mlp_label = mlp_row.predicted_label\n",
        "    rf_label = rf_row.predicted_label  # Additional model prediction\n",
        "\n",
        "    # Weighted voting\n",
        "    vote_counter = Counter()\n",
        "    vote_counter[svm_label] += 3\n",
        "    vote_counter[mlp_label] += 2\n",
        "    vote_counter[rf_label] += 1\n",
        "\n",
        "    # Majority vote\n",
        "    fused_label = vote_counter.most_common(1)[0][0]\n",
        "    fused_predictions.append({\"File_Name\": File_Name, \"fused_label\": fused_label})\n",
        "\n",
        "# Save fused predictions to a new TSV file\n",
        "fused_predictions_df = pd.DataFrame(fused_predictions)\n",
        "fused_predictions_df.to_csv(output_fused_predictions_path, sep=\"\\t\", index=False)\n",
        "print(f\"Fused predictions saved to {output_fused_predictions_path}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TGowuWgjFnG",
        "outputId": "285faa05-6f04-484b-9bb2-033ede8a4512"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fused predictions saved to /content/Audio_fusion_weight.tsv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fusion without weight**"
      ],
      "metadata": {
        "id": "cUQW9m9NpxUo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# File paths (update if necessary)\n",
        "svm_predictions_path = \"/content/SVM_audio_predictions.tsv\"\n",
        "mlp_predictions_path = \"/content/MLP_audio_predictions.tsv\"\n",
        "rf_predictions_path = \"/content/RF_audio_predictions.tsv\"  # New file\n",
        "output_fused_predictions_path = \"/content/Fused_prediction_Audio.tsv\"\n",
        "\n",
        "# Load predictions\n",
        "svm_predictions = pd.read_csv(svm_predictions_path, sep=\"\\t\")\n",
        "mlp_predictions = pd.read_csv(mlp_predictions_path, sep=\"\\t\")\n",
        "rf_predictions = pd.read_csv(rf_predictions_path, sep=\"\\t\")  # Load additional predictions\n",
        "\n",
        "# Ensure all files have the same filenames in the same order\n",
        "if not (\n",
        "    all(svm_predictions['File_Name'] == mlp_predictions['File_Name']) and\n",
        "    all(svm_predictions['File_Name'] == rf_predictions['File_Name'])\n",
        "):\n",
        "    raise ValueError(\"Mismatch in filenames between the prediction files.\")\n",
        "\n",
        "# Majority fusion without weighting\n",
        "fused_predictions = []\n",
        "for _, (svm_row, mlp_row, rf_row) in enumerate(zip(\n",
        "    svm_predictions.itertuples(),\n",
        "    mlp_predictions.itertuples(),\n",
        "    rf_predictions.itertuples()\n",
        ")):\n",
        "    File_Name = svm_row.File_Name\n",
        "    svm_label = svm_row.predicted_label\n",
        "    mlp_label = mlp_row.predicted_label\n",
        "    rf_label = rf_row.predicted_label\n",
        "\n",
        "    # Equal voting\n",
        "    vote_counter = Counter([svm_label, mlp_label, rf_label])\n",
        "\n",
        "    # Majority vote\n",
        "    fused_label = vote_counter.most_common(1)[0][0]\n",
        "    fused_predictions.append({\"File_Name\": File_Name, \"fused_label\": fused_label})\n",
        "\n",
        "# Save fused predictions to a new TSV file\n",
        "fused_predictions_df = pd.DataFrame(fused_predictions)\n",
        "fused_predictions_df.to_csv(output_fused_predictions_path, sep=\"\\t\", index=False)\n",
        "print(f\"Fused predictions saved to {output_fused_predictions_path}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LFTA9HylRRc",
        "outputId": "3e0eac24-2acb-4d7d-f316-aaf3112e3f26"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fused predictions saved to /content/Fused_prediction_Audio.tsv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert filename column to uppercase\n",
        "fused_predictions_df['File_Name'] = fused_predictions_df['File_Name'].str.upper()\n",
        "\n",
        "# Save the updated file\n",
        "output_fused_file = \"/content/Final_Fused_weight_predictions.tsv\"\n",
        "fused_predictions_df.to_csv(output_fused_file, sep=\"\\t\", index=False)\n",
        "\n",
        "print(f\"Updated fused predictions saved to {output_fused_file}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PY6R6Ctln4t1",
        "outputId": "6ad822ec-e668-4e65-af8e-82acaa3598f1"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated fused predictions saved to /content/Final_Fused_weight_predictions.tsv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final (Text+Audio)"
      ],
      "metadata": {
        "id": "mddR07SKp8MP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fusion with weight**"
      ],
      "metadata": {
        "id": "bMYnaHI2qBpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# File paths (update if necessary)\n",
        "text_predictions_path = \"/content/Text_fusion_weight.tsv\"\n",
        "audio_predictions_path = \"/content/Audio_fusion_weight.tsv\" # New file\n",
        "\n",
        "# Load predictions\n",
        "text_predictions = pd.read_csv(text_predictions_path, sep=\"\\t\")\n",
        "audio_predictions = pd.read_csv(audio_predictions_path, sep=\"\\t\")\n",
        "\n",
        "\n",
        "# Trim whitespace and ensure lowercase filenames for consistency\n",
        "text_predictions['File_Name'] = text_predictions['File_Name'].str.strip().str.lower()\n",
        "audio_predictions['File_Name'] = audio_predictions['File_Name'].str.strip().str.lower()\n",
        "\n",
        "# Get unique filenames from both files\n",
        "text_filenames = set(text_predictions['File_Name'])\n",
        "audio_filenames = set(audio_predictions['File_Name'])\n",
        "\n",
        "# Find mismatches\n",
        "only_in_text = text_filenames - audio_filenames\n",
        "only_in_audio = audio_filenames - text_filenames\n",
        "\n",
        "# Print differences\n",
        "print(f\"Filenames only in text predictions: {only_in_text}\")\n",
        "print(f\"Filenames only in audio predictions: {only_in_audio}\")\n",
        "\n",
        "# Keep only common filenames\n",
        "common_filenames = text_filenames & audio_filenames\n",
        "\n",
        "# Filter both DataFrames to keep only common filenames\n",
        "text_predictions = text_predictions[text_predictions['File_Name'].isin(common_filenames)]\n",
        "audio_predictions = audio_predictions[audio_predictions['File_Name'].isin(common_filenames)]\n",
        "\n",
        "# Sort and reset index to align both files properly\n",
        "text_predictions = text_predictions.sort_values(by='File_Name').reset_index(drop=True)\n",
        "audio_predictions = audio_predictions.sort_values(by='File_Name').reset_index(drop=True)\n",
        "\n",
        "# Define weights (adjust as needed)\n",
        "weight_text = 1\n",
        "weight_audio = 2\n",
        "\n",
        "fused_predictions = []\n",
        "for _, (text_row, audio_row) in enumerate(zip(text_predictions.itertuples(), audio_predictions.itertuples())):\n",
        "    File_Name = text_row.File_Name\n",
        "    text_label = text_row.fused_label\n",
        "    audio_label = audio_row.fused_label\n",
        "\n",
        "    # Weighted vote counting\n",
        "    vote_counter = Counter()\n",
        "    vote_counter[text_label] += weight_text\n",
        "    vote_counter[audio_label] += weight_audio\n",
        "\n",
        "    # Select the label with the highest weight\n",
        "    fused_label = vote_counter.most_common(1)[0][0]\n",
        "    fused_predictions.append({\"File_Name\": File_Name, \"fused_label\": fused_label})\n",
        "\n",
        "# Save to TSV file\n",
        "fused_predictions_df = pd.DataFrame(fused_predictions)\n",
        "output_fused_file = \"/content/SSNCSE_Tamil_Run1.tsv\"\n",
        "fused_predictions_df.to_csv(output_fused_file, sep=\"\\t\", index=False)\n",
        "\n",
        "print(f\"Final fused predictions saved to {output_fused_file}.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoNaLwvdlIPX",
        "outputId": "11010e5a-8073-46e2-e264-78d0cf6a700f"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filenames only in text predictions: set()\n",
            "Filenames only in audio predictions: set()\n",
            "Final fused predictions saved to /content/SSNCSE_Tamil_Run1.tsv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the true labels from the Excel file (Sheet1)\n",
        "true_labels_file = \"/content/TA-AT-test.xlsx - Sheet1.tsv\"\n",
        "true_labels_data = pd.read_csv(true_labels_file, sep='\\t')\n",
        "\n",
        "# Load the predicted labels from the TSV file\n",
        "predicted_labels_file = \"/content/SSNCSE_Tamil_Run1.tsv\"\n",
        "predicted_data = pd.read_csv(predicted_labels_file, sep='\\t')\n",
        "\n",
        "# Ensure both datasets have the same 'File Name' column for alignment\n",
        "# Remove leading/trailing spaces from all column names\n",
        "true_labels_data.columns = true_labels_data.columns.str.strip()\n",
        "\n",
        "# Now merge the datasets using the cleaned column names\n",
        "merged_data = pd.merge(true_labels_data[['File_Name', 'Class Label']],\n",
        "                       predicted_data[['File_Name', 'fused_label']],\n",
        "                       on='File_Name')\n",
        "\n",
        "# Extract true labels and predicted labels\n",
        "y_true = merged_data['Class Label']\n",
        "y_pred = merged_data['fused_label']\n",
        "\n",
        "# Continue with the rest of your code...\n",
        "# Encode the true and predicted labels (if needed)\n",
        "label_encoder = LabelEncoder()\n",
        "y_true_encoded = label_encoder.fit_transform(y_true)\n",
        "y_pred_encoded = label_encoder.transform(y_pred)\n",
        "\n",
        "# Generate the classification report\n",
        "print(classification_report(y_true_encoded, y_pred_encoded, target_names=label_encoder.classes_))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b9iVK4Rn9ez",
        "outputId": "3d21c9cd-e6e1-4779-b329-9af89137af19"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.33      0.30      0.32        10\n",
            "           G       0.50      0.80      0.62        10\n",
            "           N       0.80      0.40      0.53        10\n",
            "           P       0.89      0.80      0.84        10\n",
            "           R       0.64      0.70      0.67        10\n",
            "\n",
            "    accuracy                           0.60        50\n",
            "   macro avg       0.63      0.60      0.59        50\n",
            "weighted avg       0.63      0.60      0.59        50\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fusion without weight**"
      ],
      "metadata": {
        "id": "94hMiqWwqJfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# File paths (update if necessary)\n",
        "text_predictions_path = \"/content/Text_fusion.tsv\"\n",
        "audio_predictions_path = \"/content/Fused_prediction_Audio.tsv\"  # New file\n",
        "\n",
        "# Load predictions\n",
        "text_predictions = pd.read_csv(text_predictions_path, sep=\"\\t\")\n",
        "audio_predictions = pd.read_csv(audio_predictions_path, sep=\"\\t\")\n",
        "\n",
        "# Trim whitespace and ensure lowercase filenames for consistency\n",
        "text_predictions['File_Name'] = text_predictions['File_Name'].str.strip().str.lower()\n",
        "audio_predictions['File_Name'] = audio_predictions['File_Name'].str.strip().str.lower()\n",
        "\n",
        "# Get unique filenames from both files\n",
        "text_filenames = set(text_predictions['File_Name'])\n",
        "audio_filenames = set(audio_predictions['File_Name'])\n",
        "\n",
        "# Find mismatches\n",
        "only_in_text = text_filenames - audio_filenames\n",
        "only_in_audio = audio_filenames - text_filenames\n",
        "\n",
        "# Print differences\n",
        "print(f\"Filenames only in text predictions: {only_in_text}\")\n",
        "print(f\"Filenames only in audio predictions: {only_in_audio}\")\n",
        "\n",
        "# Keep only common filenames\n",
        "common_filenames = text_filenames & audio_filenames\n",
        "\n",
        "# Filter both DataFrames to keep only common filenames\n",
        "text_predictions = text_predictions[text_predictions['File_Name'].isin(common_filenames)]\n",
        "audio_predictions = audio_predictions[audio_predictions['File_Name'].isin(common_filenames)]\n",
        "\n",
        "# Sort and reset index to align both files properly\n",
        "text_predictions = text_predictions.sort_values(by='File_Name').reset_index(drop=True)\n",
        "audio_predictions = audio_predictions.sort_values(by='File_Name').reset_index(drop=True)\n",
        "\n",
        "# Majority fusion without weighting\n",
        "fused_predictions = []\n",
        "for _, (text_row, audio_row) in enumerate(zip(text_predictions.itertuples(), audio_predictions.itertuples())):\n",
        "    File_Name = text_row.File_Name\n",
        "    text_label = text_row.fused_label\n",
        "    audio_label = audio_row.fused_label\n",
        "\n",
        "    # Equal voting\n",
        "    vote_counter = Counter([text_label, audio_label])\n",
        "\n",
        "    # Select the label with the highest count\n",
        "    fused_label = vote_counter.most_common(1)[0][0]\n",
        "    fused_predictions.append({\"File_Name\": File_Name, \"fused_label\": fused_label})\n",
        "\n",
        "# Save to TSV file\n",
        "fused_predictions_df = pd.DataFrame(fused_predictions)\n",
        "output_fused_file = \"/content/SSNCSE_Tamil_Run2.tsv\"\n",
        "fused_predictions_df.to_csv(output_fused_file, sep=\"\\t\", index=False)\n",
        "\n",
        "print(f\"Final fused predictions saved to {output_fused_file}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZlzcNgPGoNFQ",
        "outputId": "798c26d9-c3f1-42bc-ec6f-71b54b2b6a1b"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filenames only in text predictions: set()\n",
            "Filenames only in audio predictions: set()\n",
            "Final fused predictions saved to /content/SSNCSE_Tamil_Run2.tsv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the true labels from the Excel file (Sheet1)\n",
        "true_labels_file = \"/content/TA-AT-test.xlsx - Sheet1.tsv\"\n",
        "true_labels_data = pd.read_csv(true_labels_file, sep='\\t')\n",
        "\n",
        "# Load the predicted labels from the TSV file\n",
        "predicted_labels_file = \"/content/SSNCSE_Tamil_Run1.tsv\"\n",
        "predicted_data = pd.read_csv(predicted_labels_file, sep='\\t')\n",
        "\n",
        "# Ensure both datasets have the same 'File Name' column for alignment\n",
        "# Remove leading/trailing spaces from all column names\n",
        "true_labels_data.columns = true_labels_data.columns.str.strip()\n",
        "\n",
        "# Now merge the datasets using the cleaned column names\n",
        "merged_data = pd.merge(true_labels_data[['File_Name', 'Class Label']],\n",
        "                       predicted_data[['File_Name', 'fused_label']],\n",
        "                       on='File_Name')\n",
        "\n",
        "# Extract true labels and predicted labels\n",
        "y_true = merged_data['Class Label']\n",
        "y_pred = merged_data['fused_label']\n",
        "\n",
        "# Continue with the rest of your code...\n",
        "# Encode the true and predicted labels (if needed)\n",
        "label_encoder = LabelEncoder()\n",
        "y_true_encoded = label_encoder.fit_transform(y_true)\n",
        "y_pred_encoded = label_encoder.transform(y_pred)\n",
        "\n",
        "# Generate the classification report\n",
        "print(classification_report(y_true_encoded, y_pred_encoded, target_names=label_encoder.classes_))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lqJvr0pfqHtA",
        "outputId": "8a484bca-a825-43c9-9925-410233c92238"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.38      0.60      0.46        10\n",
            "           G       0.70      0.70      0.70        10\n",
            "           N       0.40      0.60      0.48        10\n",
            "           P       0.00      0.00      0.00        10\n",
            "           R       0.86      0.60      0.71        10\n",
            "\n",
            "    accuracy                           0.50        50\n",
            "   macro avg       0.47      0.50      0.47        50\n",
            "weighted avg       0.47      0.50      0.47        50\n",
            "\n"
          ]
        }
      ]
    }
  ]
}
