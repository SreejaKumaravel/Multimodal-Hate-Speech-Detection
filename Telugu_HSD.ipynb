{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHMv0OLA2Mo2",
        "outputId": "b35aaa45-3323-4237-e78c-135fe12675e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text"
      ],
      "metadata": {
        "id": "RsNzZ2YeDRBb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MLP Classifier**"
      ],
      "metadata": {
        "id": "ID9rN1OSDQw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Telugu Stopwords (example - expand this list as necessary)\n",
        "telugu_stopwords = set([\n",
        "    \"ఇది\", \"అది\", \"మరియు\", \"కాని\", \"అందు\", \"కోసం\", \"పైన\", \"కింద\", \"ఈ\", \"ఎవరు\", \"మేము\",\n",
        "    \"మీ\", \"తరువాత\", \"ఎందుకు\", \"ఎక్కడ\", \"ఎప్పుడు\", \"అక్కడ\", \"ఇక్కడ\", \"అంతే\", \"అంతగా\", \"వారి\", \"కాదు\"\n",
        "])\n",
        "\n",
        "# Load the Telugu dataset (adjust the file paths as needed)\n",
        "train_data = pd.read_csv(\"/content/drive/MyDrive/Project/Telugu/Text/TE-AT-train..csv\")  # Training data with text and labels\n",
        "test_data = pd.read_csv(\"/content/drive/MyDrive/Project/Test/Telugu/TE-AT-test.csv\")  # Test data with text only\n",
        "\n",
        "# Step 1: Preprocess Telugu text\n",
        "def preprocess_text_telugu(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation, digits, and special characters, keeping only Telugu alphabets and spaces\n",
        "    text = ''.join(char for char in text if '\\u0C00' <= char <= '\\u0C7F' or char.isspace())\n",
        "    # Remove stopwords\n",
        "    text = ' '.join([word for word in text.split() if word not in telugu_stopwords])\n",
        "    return text\n",
        "\n",
        "train_data['Transcript'] = train_data['Transcript'].apply(preprocess_text_telugu)\n",
        "test_data['Transcript'] = test_data['Transcript'].apply(preprocess_text_telugu)\n",
        "\n",
        "# Step 2: Check class distribution in training data\n",
        "class_counts = train_data['Class Label Short'].value_counts()\n",
        "print(\"Class distribution:\", class_counts)\n",
        "\n",
        "# Step 3: Compute class weights for imbalanced dataset\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(train_data['Class Label Short']), y=train_data['Class Label Short'])\n",
        "class_weight_dict = dict(zip(np.unique(train_data['Class Label Short']), class_weights))\n",
        "print(\"Class weights:\", class_weight_dict)\n",
        "\n",
        "# Step 4: Prepare the text data (transcripts) and labels\n",
        "X_train = train_data['Transcript']\n",
        "y_train = train_data['Class Label Short']\n",
        "X_test = test_data['Transcript']\n",
        "\n",
        "# Convert labels to numeric format\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_numeric = label_encoder.fit_transform(y_train)\n",
        "\n",
        "# Step 5: Extract TF-IDF features from Telugu text\n",
        "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), analyzer='char', max_df=0.9, min_df=5)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Step 6: Train an MLPClassifier\n",
        "mlp_model = MLPClassifier(hidden_layer_sizes=(128, 64), activation='relu', solver='adam', max_iter=50, random_state=42)\n",
        "mlp_model.fit(X_train_tfidf, y_train_numeric)\n",
        "\n",
        "# Step 7: Predict class labels for test data\n",
        "y_test_pred = mlp_model.predict(X_test_tfidf)\n",
        "\n",
        "# Convert numeric predictions back to string labels\n",
        "y_test_pred_labels = label_encoder.inverse_transform(y_test_pred)\n",
        "\n",
        "# Step 8: Save predictions in a TSV file with 'filename' and 'Predicted_Label' columns\n",
        "test_data['Predicted_Label'] = y_test_pred_labels\n",
        "\n",
        "# Assuming 'File Name' is a column in test_data (adjust if needed)\n",
        "predictions = test_data[['File_Name', 'Predicted_Label']]\n",
        "\n",
        "# Save to TSV file\n",
        "predictions.to_csv(\"MLP_prediction.tsv\", sep='\\t', index=False)\n",
        "print(\"Predictions saved to predicted_test_data_telugu.tsv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQuETNIZ2ghF",
        "outputId": "b7a56f30-54dc-4666-834c-afe474910dc1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution: Class Label Short\n",
            "N    198\n",
            "C    122\n",
            "G    106\n",
            "R     72\n",
            "P     58\n",
            "Name: count, dtype: int64\n",
            "Class weights: {'C': 0.9114754098360656, 'G': 1.049056603773585, 'N': 0.5616161616161616, 'P': 1.9172413793103449, 'R': 1.5444444444444445}\n",
            "Predictions saved to predicted_test_data_telugu.tsv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SVM**"
      ],
      "metadata": {
        "id": "5d7ashcADOMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Telugu Stopwords (example - expand this list as necessary)\n",
        "telugu_stopwords = set([\n",
        "    \"ఇది\", \"అది\", \"మరియు\", \"కాని\", \"అందు\", \"కోసం\", \"పైన\", \"కింద\", \"ఈ\", \"ఎవరు\", \"మేము\",\n",
        "    \"మీ\", \"తరువాత\", \"ఎందుకు\", \"ఎక్కడ\", \"ఎప్పుడు\", \"అక్కడ\", \"ఇక్కడ\", \"అంతే\", \"అంతగా\", \"వారి\", \"కాదు\"\n",
        "])\n",
        "\n",
        "# Load the Telugu dataset (adjust the file paths as needed)\n",
        "train_data = pd.read_csv(\"/content/drive/MyDrive/Project/Telugu/Text/TE-AT-train..csv\")  # Training data with text and labels\n",
        "test_data = pd.read_csv(\"/content/drive/MyDrive/Project/Test/Telugu/TE-AT-test.csv\")  # Test data with text only\n",
        "\n",
        "# Step 1: Preprocess Telugu text\n",
        "def preprocess_text_telugu(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation, digits, and special characters, keeping only Telugu alphabets and spaces\n",
        "    text = ''.join(char for char in text if '\\u0C00' <= char <= '\\u0C7F' or char.isspace())\n",
        "    # Remove stopwords\n",
        "    text = ' '.join([word for word in text.split() if word not in telugu_stopwords])\n",
        "    return text\n",
        "\n",
        "train_data['Transcript'] = train_data['Transcript'].apply(preprocess_text_telugu)\n",
        "test_data['Transcript'] = test_data['Transcript'].apply(preprocess_text_telugu)\n",
        "\n",
        "# Step 2: Check class distribution in training data\n",
        "class_counts = train_data['Class Label Short'].value_counts()\n",
        "print(\"Class distribution:\", class_counts)\n",
        "\n",
        "# Step 3: Compute class weights for imbalanced dataset\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(train_data['Class Label Short']), y=train_data['Class Label Short'])\n",
        "class_weight_dict = dict(zip(np.unique(train_data['Class Label Short']), class_weights))\n",
        "print(\"Class weights:\", class_weight_dict)\n",
        "\n",
        "# Step 4: Prepare the text data (transcripts) and labels\n",
        "X_train = train_data['Transcript']\n",
        "y_train = train_data['Class Label Short']\n",
        "X_test = test_data['Transcript']\n",
        "\n",
        "# Convert labels to numeric format\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_numeric = label_encoder.fit_transform(y_train)\n",
        "\n",
        "# Step 5: Extract TF-IDF features from Telugu text\n",
        "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), analyzer='char', max_df=0.9, min_df=5)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Step 6: Train an SVM classifier\n",
        "svm_model = SVC(kernel='rbf', class_weight='balanced', random_state=42)  # Use linear kernel for text classification\n",
        "svm_model.fit(X_train_tfidf, y_train_numeric)\n",
        "\n",
        "# Step 7: Predict class labels for test data\n",
        "y_test_pred = svm_model.predict(X_test_tfidf)\n",
        "\n",
        "# Convert numeric predictions back to string labels\n",
        "y_test_pred_labels = label_encoder.inverse_transform(y_test_pred)\n",
        "\n",
        "# Step 8: Save predictions in a TSV file with 'filename' and 'Predicted_Label' columns\n",
        "test_data['Predicted_Label'] = y_test_pred_labels\n",
        "\n",
        "# Assuming 'File Name' is a column in test_data (adjust if needed)\n",
        "predictions = test_data[['File_Name', 'Predicted_Label']]\n",
        "\n",
        "# Save to TSV file\n",
        "predictions.to_csv(\"SVM_prediction.tsv\", sep='\\t', index=False)\n",
        "print(\"Predictions saved to predicted_test_data_telugu_svm_tfidf.tsv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Vf9t3zS238k",
        "outputId": "e9907879-4477-4d24-db58-408c2da26e9b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution: Class Label Short\n",
            "N    198\n",
            "C    122\n",
            "G    106\n",
            "R     72\n",
            "P     58\n",
            "Name: count, dtype: int64\n",
            "Class weights: {'C': 0.9114754098360656, 'G': 1.049056603773585, 'N': 0.5616161616161616, 'P': 1.9172413793103449, 'R': 1.5444444444444445}\n",
            "Predictions saved to predicted_test_data_telugu_svm_tfidf.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Forest**"
      ],
      "metadata": {
        "id": "NexjZ7ukDKpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Telugu Stopwords (example - expand this list as necessary)\n",
        "telugu_stopwords = set([\n",
        "    \"ఇది\", \"అది\", \"మరియు\", \"కాని\", \"అందు\", \"కోసం\", \"పైన\", \"కింద\", \"ఈ\", \"ఎవరు\", \"మేము\",\n",
        "    \"మీ\", \"తరువాత\", \"ఎందుకు\", \"ఎక్కడ\", \"ఎప్పుడు\", \"అక్కడ\", \"ఇక్కడ\", \"అంతే\", \"అంతగా\", \"వారి\", \"కాదు\"\n",
        "])\n",
        "\n",
        "# Load the Telugu dataset (adjust the file paths as needed)\n",
        "train_data = pd.read_csv(\"/content/drive/MyDrive/Project/Telugu/Text/TE-AT-train..csv\")  # Training data with text and labels\n",
        "test_data = pd.read_csv(\"/content/drive/MyDrive/Project/Test/Telugu/TE-AT-test.csv\")  # Test data with text only\n",
        "\n",
        "# Step 1: Preprocess Telugu text\n",
        "def preprocess_text_telugu(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation, digits, and special characters, keeping only Telugu alphabets and spaces\n",
        "    text = ''.join(char for char in text if '\\u0C00' <= char <= '\\u0C7F' or char.isspace())\n",
        "    # Remove stopwords\n",
        "    text = ' '.join([word for word in text.split() if word not in telugu_stopwords])\n",
        "    return text\n",
        "\n",
        "train_data['Transcript'] = train_data['Transcript'].apply(preprocess_text_telugu)\n",
        "test_data['Transcript'] = test_data['Transcript'].apply(preprocess_text_telugu)\n",
        "\n",
        "# Step 2: Check class distribution in training data\n",
        "class_counts = train_data['Class Label Short'].value_counts()\n",
        "print(\"Class distribution:\", class_counts)\n",
        "\n",
        "# Step 3: Compute class weights for imbalanced dataset\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(train_data['Class Label Short']), y=train_data['Class Label Short'])\n",
        "class_weight_dict = dict(zip(np.unique(train_data['Class Label Short']), class_weights))\n",
        "print(\"Class weights:\", class_weight_dict)\n",
        "\n",
        "# Step 4: Prepare the text data (transcripts) and labels\n",
        "X_train = train_data['Transcript']\n",
        "y_train = train_data['Class Label Short']\n",
        "X_test = test_data['Transcript']\n",
        "\n",
        "# Convert labels to numeric format\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_numeric = label_encoder.fit_transform(y_train)\n",
        "\n",
        "# Step 5: Extract TF-IDF features from Telugu text\n",
        "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2), analyzer='char', max_df=0.9, min_df=5)\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Step 6: Train a Random Forest classifier\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,  # Number of trees in the forest\n",
        "    class_weight='balanced',  # Handle imbalanced classes\n",
        "    random_state=42,\n",
        "    n_jobs=-1  # Use all available processors\n",
        ")\n",
        "rf_model.fit(X_train_tfidf, y_train_numeric)\n",
        "\n",
        "# Step 7: Predict class labels for test data\n",
        "y_test_pred = rf_model.predict(X_test_tfidf)\n",
        "\n",
        "# Convert numeric predictions back to string labels\n",
        "y_test_pred_labels = label_encoder.inverse_transform(y_test_pred)\n",
        "\n",
        "# Step 8: Save predictions in a TSV file with 'filename' and 'Predicted_Label' columns\n",
        "test_data['Predicted_Label'] = y_test_pred_labels\n",
        "\n",
        "# Assuming 'File Name' is a column in test_data (adjust if needed)\n",
        "predictions = test_data[['File_Name', 'Predicted_Label']]\n",
        "\n",
        "# Save to TSV file\n",
        "predictions.to_csv(\"Random_prediction\", sep='\\t', index=False)\n",
        "print(\"Predictions saved to predicted_test_data_telugu_rf_tfidf.tsv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrFN63Zh3Asw",
        "outputId": "1f9a9d01-9392-4352-86c7-1824799ee06f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution: Class Label Short\n",
            "N    198\n",
            "C    122\n",
            "G    106\n",
            "R     72\n",
            "P     58\n",
            "Name: count, dtype: int64\n",
            "Class weights: {'C': 0.9114754098360656, 'G': 1.049056603773585, 'N': 0.5616161616161616, 'P': 1.9172413793103449, 'R': 1.5444444444444445}\n",
            "Predictions saved to predicted_test_data_telugu_rf_tfidf.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the TSV file\n",
        "file_path = \"/content/TE-AT-test.xlsx - Sheet1.tsv\"\n",
        "df = pd.read_csv(file_path, sep=\"\\t\")\n",
        "\n",
        "# Change column name (e.g., from 'Old_Column_Name' to 'New_Column_Name')\n",
        "df.rename(columns={\"File Name\": \"File_Name\"}, inplace=True)\n",
        "\n",
        "# Save the updated DataFrame back to the TSV file\n",
        "output_path = \"/content/TE-AT-test.xlsx - Sheet1.tsv\"\n",
        "df.to_csv(output_path, sep=\"\\t\", index=False)\n",
        "\n",
        "print(f\"Updated file saved to {output_path}\")\n"
      ],
      "metadata": {
        "id": "VIy6nzy03cZJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fusion with weight**"
      ],
      "metadata": {
        "id": "T4NLY9_uDDCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# File paths (update if necessary)\n",
        "random_predictions_path = \"/content/Random_prediction\"\n",
        "mlp_predictions_path = \"/content/MLP_prediction.tsv\"\n",
        "svm_predictions_path = \"/content/SVM_prediction.tsv\"  # New file\n",
        "output_fused_predictions_path =\"/content/Text_fusion_weight.tsv\"\n",
        "\n",
        "\n",
        "# Load predictions\n",
        "random_predictions = pd.read_csv(random_predictions_path, sep=\"\\t\")\n",
        "mlp_predictions = pd.read_csv(mlp_predictions_path, sep=\"\\t\")\n",
        "svm_predictions = pd.read_csv(svm_predictions_path, sep=\"\\t\")  # Load additional predictions\n",
        "\n",
        "# Ensure all files have the same filenames in the same order\n",
        "if not (\n",
        "    all(random_predictions['File_Name'] == mlp_predictions['File_Name']) and\n",
        "    all(random_predictions['File_Name'] == svm_predictions['File_Name'])\n",
        "):\n",
        "    raise ValueError(\"Mismatch in filenames between the prediction files.\")\n",
        "\n",
        "# Majority fusion with different weights\n",
        "fused_predictions = []\n",
        "for _, (ran_row, mlp_row, svm_row) in enumerate(zip(\n",
        "    random_predictions.itertuples(),\n",
        "    mlp_predictions.itertuples(),\n",
        "    svm_predictions.itertuples()\n",
        ")):\n",
        "    File_Name = ran_row.File_Name\n",
        "    ran_label = ran_row.Predicted_Label\n",
        "    mlp_label = mlp_row.Predicted_Label\n",
        "    svm_label = svm_row.Predicted_Label\n",
        "\n",
        "    # Weighted voting\n",
        "    vote_counter = Counter()\n",
        "    vote_counter[ran_label] += 1\n",
        "    vote_counter[mlp_label] +=  3\n",
        "    vote_counter[svm_label] += 2\n",
        "    # Majority vote\n",
        "    fused_label = vote_counter.most_common(1)[0][0]\n",
        "    fused_predictions.append({\"File_Name\": File_Name, \"fused_label\": fused_label})\n",
        "\n",
        "# Save fused predictions to a new TSV file\n",
        "fused_predictions_df = pd.DataFrame(fused_predictions)\n",
        "fused_predictions_df.to_csv(output_fused_predictions_path, sep=\"\\t\", index=False)\n",
        "print(f\"Fused predictions saved to {output_fused_predictions_path}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEkHxa9g3tRS",
        "outputId": "abf8f022-f710-486a-8c8b-c5e0ea3393f7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fused predictions saved to /content/Text_fusion_weight.tsv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fusion without weight**"
      ],
      "metadata": {
        "id": "9ncQ09-aDGZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# File paths (update if necessary)\n",
        "random_predictions_path = \"/content/Random_prediction\"\n",
        "mlp_predictions_path = \"/content/MLP_prediction.tsv\"\n",
        "svm_predictions_path = \"/content/SVM_prediction.tsv\"  # New file\n",
        "output_fused_predictions_path = \"/content/Text_fusion.tsv\"\n",
        "\n",
        "# Load predictions\n",
        "random_predictions = pd.read_csv(random_predictions_path, sep=\"\\t\")\n",
        "mlp_predictions = pd.read_csv(mlp_predictions_path, sep=\"\\t\")\n",
        "svm_predictions = pd.read_csv(svm_predictions_path, sep=\"\\t\")\n",
        "\n",
        "# Ensure all files have the same filenames in the same order\n",
        "if not (\n",
        "    all(random_predictions['File_Name'] == mlp_predictions['File_Name']) and\n",
        "    all(random_predictions['File_Name'] == svm_predictions['File_Name'])\n",
        "):\n",
        "    raise ValueError(\"Mismatch in filenames between the prediction files.\")\n",
        "\n",
        "# Majority fusion without weighting\n",
        "fused_predictions = []\n",
        "for _, (ran_row, mlp_row, svm_row) in enumerate(zip(\n",
        "    random_predictions.itertuples(),\n",
        "    mlp_predictions.itertuples(),\n",
        "    svm_predictions.itertuples()\n",
        ")):\n",
        "    File_Name = ran_row.File_Name\n",
        "    ran_label = ran_row.Predicted_Label\n",
        "    mlp_label = mlp_row.Predicted_Label\n",
        "    svm_label = svm_row.Predicted_Label\n",
        "\n",
        "    # Equal voting\n",
        "    vote_counter = Counter([ran_label, mlp_label, svm_label])\n",
        "\n",
        "    # Majority vote\n",
        "    fused_label = vote_counter.most_common(1)[0][0]\n",
        "    fused_predictions.append({\"File_Name\": File_Name, \"fused_label\": fused_label})\n",
        "\n",
        "# Save fused predictions to a new TSV file\n",
        "fused_predictions_df = pd.DataFrame(fused_predictions)\n",
        "fused_predictions_df.to_csv(output_fused_predictions_path, sep=\"\\t\", index=False)\n",
        "print(f\"Fused predictions saved to {output_fused_predictions_path}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ge9ggn9E4WWO",
        "outputId": "fb7c84f0-6ee2-4697-cb5a-3730f9a2edda"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fused predictions saved to /content/Text_fusion.tsv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Audio"
      ],
      "metadata": {
        "id": "lbLiSp1w5AB9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MLP classifier**"
      ],
      "metadata": {
        "id": "GXzIs9cQ5BX2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import librosa\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Paths (update with actual paths)\n",
        "train_csv_path = \"/content/drive/MyDrive/Project/Telugu/Text/TE-AT-train..csv\"\n",
        "train_audio_dir = \"/content/drive/MyDrive/Project/Telugu/audio\"  # Update to Telugu audio directory\n",
        "test_audio_dir = \"/content/drive/MyDrive/Project/Test/Telugu/audio\"  # Telugu test audio directory\n",
        "output_tsv_path = \"MLP_audio_predictions.tsv\"  # Change path as needed\n",
        "\n",
        "# Step 1: Load train.csv to map class labels\n",
        "train_df = pd.read_csv(train_csv_path)\n",
        "train_df['Class Label Short'] = train_df['Class Label Short'].astype(str)\n",
        "\n",
        "# Step 2: Preprocessing Telugu audio (Log-Mel Spectrogram feature extraction)\n",
        "def preprocess_audio(file_path, sr=16000, n_mels=128):\n",
        "    try:\n",
        "        # Load the audio file with the original sample rate\n",
        "        y, original_sr = librosa.load(file_path, sr=None)\n",
        "\n",
        "        # Resample the audio to the desired sample rate if necessary\n",
        "        if original_sr != sr:\n",
        "            y = librosa.resample(y, orig_sr=original_sr, target_sr=sr)\n",
        "\n",
        "        # Trim silence from the beginning and end of the audio\n",
        "        y, _ = librosa.effects.trim(y)\n",
        "\n",
        "        # Extract Log-Mel Spectrogram\n",
        "        mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, fmax=sr // 2)\n",
        "        log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
        "\n",
        "        # Return the mean of Log-Mel Spectrogram across time frames to get a fixed-length feature vector\n",
        "        return np.mean(log_mel_spectrogram, axis=1)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Step 3: Extract features and labels for training\n",
        "X_train, y_train = [], []\n",
        "for _, row in train_df.iterrows():\n",
        "    file_path = os.path.join(train_audio_dir, row['File_Name'] + \".wav\")  # Add .wav extension\n",
        "    if os.path.exists(file_path):\n",
        "        features = preprocess_audio(file_path)\n",
        "        if features is not None:\n",
        "            X_train.append(features)\n",
        "            y_train.append(row['Class Label Short'])\n",
        "        else:\n",
        "            print(f\"Skipping file {file_path} due to feature extraction failure.\")\n",
        "    else:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "\n",
        "# Check if X_train is empty\n",
        "if X_train.size == 0:\n",
        "    print(\"No valid training data found. Please check file paths and data.\")\n",
        "else:\n",
        "    # Proceed with model training\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "\n",
        "    # Step 4: Compute class weights\n",
        "    class_weights = compute_class_weight(\n",
        "        class_weight=\"balanced\",\n",
        "        classes=np.unique(y_train_encoded),\n",
        "        y=y_train_encoded\n",
        "    )\n",
        "    class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "    print(f\"Class weights: {class_weight_dict}\")\n",
        "\n",
        "    # Step 5: Train MLPClassifier\n",
        "    clf = MLPClassifier(\n",
        "        hidden_layer_sizes=(128, 64),\n",
        "        activation='relu',\n",
        "        solver='adam',\n",
        "        max_iter=200,\n",
        "        random_state=42\n",
        "    )\n",
        "    clf.fit(X_train, y_train_encoded)\n",
        "    print(\"MLPClassifier trained successfully.\")\n",
        "\n",
        "# Step 6: Predict for test data\n",
        "test_predictions = []\n",
        "test_files = [f for f in os.listdir(test_audio_dir) if f.endswith('.wav')]\n",
        "for file_name in test_files:\n",
        "    file_path = os.path.join(test_audio_dir, file_name)\n",
        "    features = preprocess_audio(file_path)\n",
        "    if features is not None:\n",
        "        predicted_label_encoded = clf.predict([features])\n",
        "        predicted_label = label_encoder.inverse_transform(predicted_label_encoded)[0]\n",
        "        test_predictions.append({\"File_Name\": file_name, \"predicted_label\": predicted_label})\n",
        "    else:\n",
        "        print(f\"Skipping file {file_path} due to feature extraction failure.\")\n",
        "\n",
        "# Step 7: Save predictions to a TSV file\n",
        "test_predictions_df = pd.DataFrame(test_predictions)\n",
        "test_predictions_df.to_csv(output_tsv_path, sep=\"\\t\", index=False)\n",
        "print(f\"Predictions saved to {output_tsv_path}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCkniFas47kh",
        "outputId": "bd55b27c-28ad-49eb-95ff-831b0e2a2a6e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File not found: /content/drive/MyDrive/Project/Telugu/audio/H_TE_002_G_F_041_005.wav\n",
            "File not found: /content/drive/MyDrive/Project/Telugu/audio/H_TE_002_G_F_041_006.wav\n",
            "File not found: /content/drive/MyDrive/Project/Telugu/audio/H_TE_002_G_F_042_001.wav\n",
            "File not found: /content/drive/MyDrive/Project/Telugu/audio/H_TE_002_G_F_043_002.wav\n",
            "File not found: /content/drive/MyDrive/Project/Telugu/audio/H_TE_002_G_F_043_003.wav\n",
            "Class weights: {0: 0.9032786885245901, 1: 1.0910891089108912, 2: 0.5565656565656566, 3: 1.9, 4: 1.5305555555555554}\n",
            "MLPClassifier trained successfully.\n",
            "Predictions saved to MLP_audio_predictions.tsv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Forest**"
      ],
      "metadata": {
        "id": "aUsZk4Wm6706"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import librosa\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Paths (update with actual paths)\n",
        "train_csv_path = \"/content/drive/MyDrive/Project/Telugu/Text/TE-AT-train..csv\"\n",
        "train_audio_dir = \"/content/drive/MyDrive/Project/Telugu/audio\"  # Update to Telugu audio directory\n",
        "test_audio_dir = \"/content/drive/MyDrive/Project/Test/Telugu/audio\"  # Telugu test audio directory\n",
        "output_tsv_path = \"Random_audio_predictions.tsv\"  # Change path as needed\n",
        "\n",
        "# Step 1: Load train.csv to map class labels\n",
        "train_df = pd.read_csv(train_csv_path)\n",
        "train_df['Class Label Short'] = train_df['Class Label Short'].astype(str)\n",
        "\n",
        "# Step 2: Preprocessing Telugu audio (Log-Mel Spectrogram feature extraction)\n",
        "def preprocess_audio(file_path, sr=16000, n_mels=128):\n",
        "    try:\n",
        "        # Load the audio file with the original sample rate\n",
        "        y, original_sr = librosa.load(file_path, sr=None)\n",
        "        # Resample the audio to the desired sample rate if necessary\n",
        "        if original_sr != sr:\n",
        "            y = librosa.resample(y, orig_sr=original_sr, target_sr=sr)\n",
        "        # Trim silence from the beginning and end of the audio\n",
        "        y, _ = librosa.effects.trim(y)\n",
        "        # Extract Log-Mel Spectrogram\n",
        "        mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, fmax=sr // 2)\n",
        "        log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
        "        # Return the mean of Log-Mel Spectrogram across time frames to get a fixed-length feature vector\n",
        "        return np.mean(log_mel_spectrogram, axis=1)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Step 3: Extract features and labels for training\n",
        "X_train, y_train = [], []\n",
        "for _, row in train_df.iterrows():\n",
        "    file_path = os.path.join(train_audio_dir, row['File_Name'] + \".wav\")  # Add .wav extension\n",
        "    if os.path.exists(file_path):\n",
        "        features = preprocess_audio(file_path)\n",
        "        if features is not None:\n",
        "            X_train.append(features)\n",
        "            y_train.append(row['Class Label Short'])\n",
        "        else:\n",
        "            print(f\"Skipping file {file_path} due to feature extraction failure.\")\n",
        "    else:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "\n",
        "# Check if X_train is empty\n",
        "if X_train.size == 0:\n",
        "    print(\"No valid training data found. Please check file paths and data.\")\n",
        "else:\n",
        "    # Proceed with model training\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "\n",
        "    # Step 4: Compute class weights\n",
        "    class_weights = compute_class_weight(\n",
        "        class_weight=\"balanced\",\n",
        "        classes=np.unique(y_train_encoded),\n",
        "        y=y_train_encoded\n",
        "    )\n",
        "    class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "    print(f\"Class weights: {class_weight_dict}\")\n",
        "\n",
        "    # Step 5: Train RandomForestClassifier\n",
        "    clf = RandomForestClassifier(\n",
        "        n_estimators=100,  # Number of trees in the forest\n",
        "        class_weight=\"balanced\",  # Handle imbalanced classes\n",
        "        random_state=42\n",
        "    )\n",
        "    clf.fit(X_train, y_train_encoded)\n",
        "    print(\"RandomForestClassifier trained successfully.\")\n",
        "\n",
        "# Step 6: Predict for test data\n",
        "test_predictions = []\n",
        "test_files = [f for f in os.listdir(test_audio_dir) if f.endswith('.wav')]\n",
        "for file_name in test_files:\n",
        "    file_path = os.path.join(test_audio_dir, file_name)\n",
        "    features = preprocess_audio(file_path)\n",
        "    if features is not None:\n",
        "        predicted_label_encoded = clf.predict([features])\n",
        "        predicted_label = label_encoder.inverse_transform(predicted_label_encoded)[0]\n",
        "        test_predictions.append({\"File_Name\": file_name, \"predicted_label\": predicted_label})\n",
        "    else:\n",
        "        print(f\"Skipping file {file_path} due to feature extraction failure.\")\n",
        "\n",
        "# Step 7: Save predictions to a TSV file\n",
        "test_predictions_df = pd.DataFrame(test_predictions)\n",
        "test_predictions_df.to_csv(output_tsv_path, sep=\"\\t\", index=False)\n",
        "print(f\"Predictions saved to {output_tsv_path}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwPWtQxT6qIR",
        "outputId": "4693b203-e43e-4034-826b-cb1913f472ab"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File not found: /content/drive/MyDrive/Project/Telugu/audio/H_TE_002_G_F_041_005.wav\n",
            "File not found: /content/drive/MyDrive/Project/Telugu/audio/H_TE_002_G_F_041_006.wav\n",
            "File not found: /content/drive/MyDrive/Project/Telugu/audio/H_TE_002_G_F_042_001.wav\n",
            "File not found: /content/drive/MyDrive/Project/Telugu/audio/H_TE_002_G_F_043_002.wav\n",
            "File not found: /content/drive/MyDrive/Project/Telugu/audio/H_TE_002_G_F_043_003.wav\n",
            "Class weights: {0: 0.9032786885245901, 1: 1.0910891089108912, 2: 0.5565656565656566, 3: 1.9, 4: 1.5305555555555554}\n",
            "RandomForestClassifier trained successfully.\n",
            "Predictions saved to Random_audio_predictions.tsv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SVM**"
      ],
      "metadata": {
        "id": "I927Apz2C8uY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import librosa\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Paths (update with actual paths)\n",
        "train_csv_path = \"/content/drive/MyDrive/Project/Telugu/Text/TE-AT-train..csv\"\n",
        "train_audio_dir = \"/content/drive/MyDrive/Project/Telugu/audio\"  # Update to Telugu audio directory\n",
        "test_audio_dir = \"/content/drive/MyDrive/Project/Test/Telugu/audio\"  # Telugu test audio directory\n",
        "output_tsv_path = \"SVM_audio_predictions.tsv\"  # Change path as needed\n",
        "\n",
        "# Step 1: Load train.csv to map class labels\n",
        "train_df = pd.read_csv(train_csv_path)\n",
        "train_df['Class Label Short'] = train_df['Class Label Short'].astype(str)\n",
        "\n",
        "# Step 2: Preprocessing Telugu audio (Log-Mel Spectrogram feature extraction)\n",
        "def preprocess_audio(file_path, sr=16000, n_mels=128):\n",
        "    try:\n",
        "        # Load the audio file\n",
        "        y, original_sr = librosa.load(file_path, sr=None)\n",
        "        # Resample if necessary\n",
        "        if original_sr != sr:\n",
        "            y = librosa.resample(y, orig_sr=original_sr, target_sr=sr)\n",
        "        # Trim silence\n",
        "        y, _ = librosa.effects.trim(y)\n",
        "        # Extract Log-Mel Spectrogram\n",
        "        mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, fmax=sr // 2)\n",
        "        log_mel_spectrogram = librosa.power_to_db(mel_spectrogram, ref=np.max)\n",
        "        # Return mean of Log-Mel Spectrogram as feature vector\n",
        "        return np.mean(log_mel_spectrogram, axis=1)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Step 3: Extract features and labels for training\n",
        "X_train, y_train = [], []\n",
        "for _, row in train_df.iterrows():\n",
        "    file_path = os.path.join(train_audio_dir, row['File_Name'] + \".wav\")  # Add .wav extension\n",
        "    if os.path.exists(file_path):\n",
        "        features = preprocess_audio(file_path)\n",
        "        if features is not None:\n",
        "            X_train.append(features)\n",
        "            y_train.append(row['Class Label Short'])\n",
        "        else:\n",
        "            print(f\"Skipping file {file_path} due to feature extraction failure.\")\n",
        "    else:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "\n",
        "# Check if training data is valid\n",
        "if X_train.size == 0:\n",
        "    print(\"No valid training data found. Please check file paths and data.\")\n",
        "else:\n",
        "    # Encode class labels\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "\n",
        "    # Scale the features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "\n",
        "    # Step 4: Compute class weights\n",
        "    class_weights = compute_class_weight(\n",
        "        class_weight=\"balanced\",\n",
        "        classes=np.unique(y_train_encoded),\n",
        "        y=y_train_encoded\n",
        "    )\n",
        "    class_weight_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
        "    print(f\"Class weights: {class_weight_dict}\")\n",
        "\n",
        "    # Step 5: Train SVM Model\n",
        "    svm_model = SVC(kernel='rbf', C=1.0, class_weight=class_weight_dict, probability=True, random_state=42)\n",
        "    svm_model.fit(X_train_scaled, y_train_encoded)\n",
        "    print(\"SVM model trained successfully.\")\n",
        "\n",
        "# Step 6: Predict for test data\n",
        "test_predictions = []\n",
        "test_files = [f for f in os.listdir(test_audio_dir) if f.endswith('.wav')]\n",
        "\n",
        "for file_name in test_files:\n",
        "    file_path = os.path.join(test_audio_dir, file_name)\n",
        "    features = preprocess_audio(file_path)\n",
        "    if features is not None:\n",
        "        features_scaled = scaler.transform([features])  # Scale test data\n",
        "        predicted_label_encoded = svm_model.predict(features_scaled)\n",
        "        predicted_label = label_encoder.inverse_transform(predicted_label_encoded)[0]\n",
        "        test_predictions.append({\"File_Name\": file_name, \"predicted_label\": predicted_label})\n",
        "    else:\n",
        "        print(f\"Skipping file {file_path} due to feature extraction failure.\")\n",
        "\n",
        "# Step 7: Save predictions to a TSV file\n",
        "test_predictions_df = pd.DataFrame(test_predictions)\n",
        "test_predictions_df.to_csv(output_tsv_path, sep=\"\\t\", index=False)\n",
        "print(f\"Predictions saved to {output_tsv_path}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1SsJNQ76_nk",
        "outputId": "3ff16bfc-959b-4295-e55d-89e1b5d71666"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File not found: /content/drive/MyDrive/Project/Telugu/audio/H_TE_002_G_F_041_005.wav\n",
            "File not found: /content/drive/MyDrive/Project/Telugu/audio/H_TE_002_G_F_041_006.wav\n",
            "File not found: /content/drive/MyDrive/Project/Telugu/audio/H_TE_002_G_F_042_001.wav\n",
            "File not found: /content/drive/MyDrive/Project/Telugu/audio/H_TE_002_G_F_043_002.wav\n",
            "File not found: /content/drive/MyDrive/Project/Telugu/audio/H_TE_002_G_F_043_003.wav\n",
            "Class weights: {0: 0.9032786885245901, 1: 1.0910891089108912, 2: 0.5565656565656566, 3: 1.9, 4: 1.5305555555555554}\n",
            "SVM model trained successfully.\n",
            "Predictions saved to SVM_audio_predictions.tsv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the TSV file\n",
        "file_path = \"/content/MLP_audio_predictions.tsv\"\n",
        "df = pd.read_csv(file_path, sep=\"\\t\")\n",
        "\n",
        "# Remove the '.wav' extension from the 'filename' column\n",
        "df['File_Name'] = df['File_Name'].str.replace('.wav', '', regex=False)\n",
        "\n",
        "# Save the updated DataFrame back to a new TSV file\n",
        "output_path = \"/content/MLP_audio_predictions.tsv\"\n",
        "df.to_csv(output_path, sep=\"\\t\", index=False)\n",
        "\n",
        "print(f\"Updated file saved to {output_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dm_mcFZl9z5B",
        "outputId": "d098c36a-b17c-4446-bbcd-1a052510598b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated file saved to /content/MLP_audio_predictions.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fusion with weight**"
      ],
      "metadata": {
        "id": "XsDrsne6C1HJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# File paths (update if necessary)\n",
        "svm_predictions_path = \"/content/SVM_audio_predictions.tsv\"\n",
        "mlp_predictions_path = \"/content/MLP_audio_predictions.tsv\"\n",
        "rf_predictions_path = \"/content/Random_audio_predictions.tsv\"  # New file\n",
        "output_fused_predictions_path =\"/content/Audio_fusion_weight.tsv\"\n",
        "\n",
        "\n",
        "# Load predictions\n",
        "svm_predictions = pd.read_csv(svm_predictions_path, sep=\"\\t\")\n",
        "mlp_predictions = pd.read_csv(mlp_predictions_path, sep=\"\\t\")\n",
        "rf_predictions = pd.read_csv(rf_predictions_path, sep=\"\\t\")  # Load additional predictions\n",
        "\n",
        "# Ensure all files have the same filenames in the same order\n",
        "if not (\n",
        "    all(svm_predictions['File_Name'] == mlp_predictions['File_Name']) and\n",
        "    all(svm_predictions['File_Name'] == rf_predictions['File_Name'])\n",
        "):\n",
        "    raise ValueError(\"Mismatch in filenames between the prediction files.\")\n",
        "\n",
        "# Majority fusion with different weights\n",
        "fused_predictions = []\n",
        "for _, (svm_row, mlp_row, rf_row) in enumerate(zip(\n",
        "    svm_predictions.itertuples(),\n",
        "    mlp_predictions.itertuples(),\n",
        "    rf_predictions.itertuples()\n",
        ")):\n",
        "    File_Name = svm_row.File_Name\n",
        "    svm_label = svm_row.predicted_label\n",
        "    mlp_label = mlp_row.predicted_label\n",
        "    rf_label = rf_row.predicted_label  # Additional model prediction\n",
        "\n",
        "    # Weighted voting\n",
        "    vote_counter = Counter()\n",
        "    vote_counter[svm_label] += 2\n",
        "    vote_counter[mlp_label] += 3\n",
        "    vote_counter[rf_label] += 1\n",
        "\n",
        "    # Majority vote\n",
        "    fused_label = vote_counter.most_common(1)[0][0]\n",
        "    fused_predictions.append({\"File_Name\": File_Name, \"fused_label\": fused_label})\n",
        "\n",
        "# Save fused predictions to a new TSV file\n",
        "fused_predictions_df = pd.DataFrame(fused_predictions)\n",
        "fused_predictions_df.to_csv(output_fused_predictions_path, sep=\"\\t\", index=False)\n",
        "print(f\"Fused predictions saved to {output_fused_predictions_path}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfGQJllC81Ym",
        "outputId": "18fbc66d-6f36-4e7a-a6fd-1244d740a137"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fused predictions saved to /content/Audio_fusion_weight.tsv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fusion without weight**"
      ],
      "metadata": {
        "id": "HuqWbx-YC4zY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# File paths (update if necessary)\n",
        "svm_predictions_path = \"/content/SVM_audio_predictions.tsv\"\n",
        "mlp_predictions_path = \"/content/MLP_audio_predictions.tsv\"\n",
        "rf_predictions_path = \"/content/Random_audio_predictions.tsv\"  # New file\n",
        "output_fused_predictions_path = \"/content/Fused_prediction_Audio.tsv\"\n",
        "\n",
        "# Load predictions\n",
        "svm_predictions = pd.read_csv(svm_predictions_path, sep=\"\\t\")\n",
        "mlp_predictions = pd.read_csv(mlp_predictions_path, sep=\"\\t\")\n",
        "rf_predictions = pd.read_csv(rf_predictions_path, sep=\"\\t\")  # Load additional predictions\n",
        "\n",
        "# Ensure all files have the same filenames in the same order\n",
        "if not (\n",
        "    all(svm_predictions['File_Name'] == mlp_predictions['File_Name']) and\n",
        "    all(svm_predictions['File_Name'] == rf_predictions['File_Name'])\n",
        "):\n",
        "    raise ValueError(\"Mismatch in filenames between the prediction files.\")\n",
        "\n",
        "# Majority fusion without weighting\n",
        "fused_predictions = []\n",
        "for _, (svm_row, mlp_row, rf_row) in enumerate(zip(\n",
        "    svm_predictions.itertuples(),\n",
        "    mlp_predictions.itertuples(),\n",
        "    rf_predictions.itertuples()\n",
        ")):\n",
        "    File_Name = svm_row.File_Name\n",
        "    svm_label = svm_row.predicted_label\n",
        "    mlp_label = mlp_row.predicted_label\n",
        "    rf_label = rf_row.predicted_label\n",
        "\n",
        "    # Equal voting\n",
        "    vote_counter = Counter([svm_label, mlp_label, rf_label])\n",
        "\n",
        "    # Majority vote\n",
        "    fused_label = vote_counter.most_common(1)[0][0]\n",
        "    fused_predictions.append({\"File_Name\": File_Name, \"fused_label\": fused_label})\n",
        "\n",
        "# Save fused predictions to a new TSV file\n",
        "fused_predictions_df = pd.DataFrame(fused_predictions)\n",
        "fused_predictions_df.to_csv(output_fused_predictions_path, sep=\"\\t\", index=False)\n",
        "print(f\"Fused predictions saved to {output_fused_predictions_path}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTiepZTj-KYU",
        "outputId": "30ced4dc-8c36-4c2c-892f-03fff2ef917e"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fused predictions saved to /content/Fused_prediction_Audio.tsv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert filename column to uppercase\n",
        "fused_predictions_df['File_Name'] = fused_predictions_df['File_Name'].str.upper()\n",
        "\n",
        "# Save the updated file\n",
        "output_fused_file = \"/content/SSNCSE_Telugu_Run2.tsv\"\n",
        "fused_predictions_df.to_csv(output_fused_file, sep=\"\\t\", index=False)\n",
        "\n",
        "print(f\"Updated fused predictions saved to {output_fused_file}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ou_zP81n-Q3W",
        "outputId": "baae40df-8496-4be3-8c4b-a48639a1a7f9"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Updated fused predictions saved to /content/SSNCSE_Telugu_Run2.tsv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Final (Text+Audio)"
      ],
      "metadata": {
        "id": "WtS1bgncDZGg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fusion with weight**"
      ],
      "metadata": {
        "id": "5c7rz8vuCwDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# File paths (update if necessary)\n",
        "text_predictions_path = \"/content/Text_fusion_weight.tsv\"\n",
        "audio_predictions_path = \"/content/Audio_fusion_weight.tsv\" # New file\n",
        "\n",
        "# Load predictions\n",
        "text_predictions = pd.read_csv(text_predictions_path, sep=\"\\t\")\n",
        "audio_predictions = pd.read_csv(audio_predictions_path, sep=\"\\t\")\n",
        "\n",
        "\n",
        "# Trim whitespace and ensure lowercase filenames for consistency\n",
        "text_predictions['File_Name'] = text_predictions['File_Name'].str.strip().str.lower()\n",
        "audio_predictions['File_Name'] = audio_predictions['File_Name'].str.strip().str.lower()\n",
        "\n",
        "# Get unique filenames from both files\n",
        "text_filenames = set(text_predictions['File_Name'])\n",
        "audio_filenames = set(audio_predictions['File_Name'])\n",
        "\n",
        "# Find mismatches\n",
        "only_in_text = text_filenames - audio_filenames\n",
        "only_in_audio = audio_filenames - text_filenames\n",
        "\n",
        "# Print differences\n",
        "print(f\"Filenames only in text predictions: {only_in_text}\")\n",
        "print(f\"Filenames only in audio predictions: {only_in_audio}\")\n",
        "\n",
        "# Keep only common filenames\n",
        "common_filenames = text_filenames & audio_filenames\n",
        "\n",
        "# Filter both DataFrames to keep only common filenames\n",
        "text_predictions = text_predictions[text_predictions['File_Name'].isin(common_filenames)]\n",
        "audio_predictions = audio_predictions[audio_predictions['File_Name'].isin(common_filenames)]\n",
        "\n",
        "# Sort and reset index to align both files properly\n",
        "text_predictions = text_predictions.sort_values(by='File_Name').reset_index(drop=True)\n",
        "audio_predictions = audio_predictions.sort_values(by='File_Name').reset_index(drop=True)\n",
        "\n",
        "# Define weights (adjust as needed)\n",
        "weight_text = 2\n",
        "weight_audio = 1\n",
        "\n",
        "fused_predictions = []\n",
        "for _, (text_row, audio_row) in enumerate(zip(text_predictions.itertuples(), audio_predictions.itertuples())):\n",
        "    File_Name = text_row.File_Name\n",
        "    text_label = text_row.fused_label\n",
        "    audio_label = audio_row.fused_label\n",
        "\n",
        "    # Weighted vote counting\n",
        "    vote_counter = Counter()\n",
        "    vote_counter[text_label] += weight_text\n",
        "    vote_counter[audio_label] += weight_audio\n",
        "\n",
        "    # Select the label with the highest weight\n",
        "    fused_label = vote_counter.most_common(1)[0][0]\n",
        "    fused_predictions.append({\"File_Name\": File_Name, \"fused_label\": fused_label})\n",
        "\n",
        "# Save to TSV file\n",
        "fused_predictions_df = pd.DataFrame(fused_predictions)\n",
        "output_fused_file = \"/content/SSNCSE_Telugu_Run1.tsv\"\n",
        "fused_predictions_df.to_csv(output_fused_file, sep=\"\\t\", index=False)\n",
        "\n",
        "print(f\"Final fused predictions saved to {output_fused_file}.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNCBqSMx-U0N",
        "outputId": "e47f5154-98cf-4727-8741-1ab523dee3b8"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filenames only in text predictions: set()\n",
            "Filenames only in audio predictions: set()\n",
            "Final fused predictions saved to /content/SSNCSE_Telugu_Run1.tsv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the true labels from the Excel file (Sheet1)\n",
        "true_labels_file = \"/content/TE-AT-test.xlsx - Sheet1.tsv\"\n",
        "true_labels_data = pd.read_csv(true_labels_file, sep='\\t')\n",
        "\n",
        "# Load the predicted labels from the TSV file\n",
        "predicted_labels_file = \"/content/SSNCSE_Telugu_Run1.tsv\"\n",
        "predicted_data = pd.read_csv(predicted_labels_file, sep='\\t')\n",
        "\n",
        "# Ensure both datasets have the same 'File Name' column for alignment\n",
        "# Remove leading/trailing spaces from all column names\n",
        "true_labels_data.columns = true_labels_data.columns.str.strip()\n",
        "\n",
        "# Now merge the datasets using the cleaned column names\n",
        "merged_data = pd.merge(true_labels_data[['File_Name', 'Class Label']],\n",
        "                       predicted_data[['File_Name', 'fused_label']],\n",
        "                       on='File_Name')\n",
        "\n",
        "# Extract true labels and predicted labels\n",
        "y_true = merged_data['Class Label']\n",
        "y_pred = merged_data['fused_label']\n",
        "\n",
        "# Continue with the rest of your code...\n",
        "# Encode the true and predicted labels (if needed)\n",
        "label_encoder = LabelEncoder()\n",
        "y_true_encoded = label_encoder.fit_transform(y_true)\n",
        "y_pred_encoded = label_encoder.transform(y_pred)\n",
        "\n",
        "# Generate the classification report\n",
        "print(classification_report(y_true_encoded, y_pred_encoded, target_names=label_encoder.classes_))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4ckVgqv-c8w",
        "outputId": "5b4dbdcf-b0db-426f-8bd7-b7ee3867c849"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.67      0.60      0.63        10\n",
            "           G       0.00      0.00      0.00        10\n",
            "           N       0.35      0.80      0.48        10\n",
            "           P       0.00      0.00      0.00        10\n",
            "           R       0.62      0.50      0.56        10\n",
            "\n",
            "    accuracy                           0.38        50\n",
            "   macro avg       0.33      0.38      0.33        50\n",
            "weighted avg       0.33      0.38      0.33        50\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fusion without weight**"
      ],
      "metadata": {
        "id": "Wl-Lcoz6CrB7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# File paths (update if necessary)\n",
        "text_predictions_path = \"/content/Text_fusion.tsv\"\n",
        "audio_predictions_path = \"/content/Fused_prediction_Audio.tsv\"  # New file\n",
        "\n",
        "# Load predictions\n",
        "text_predictions = pd.read_csv(text_predictions_path, sep=\"\\t\")\n",
        "audio_predictions = pd.read_csv(audio_predictions_path, sep=\"\\t\")\n",
        "\n",
        "# Trim whitespace and ensure lowercase filenames for consistency\n",
        "text_predictions['File_Name'] = text_predictions['File_Name'].str.strip().str.lower()\n",
        "audio_predictions['File_Name'] = audio_predictions['File_Name'].str.strip().str.lower()\n",
        "\n",
        "# Get unique filenames from both files\n",
        "text_filenames = set(text_predictions['File_Name'])\n",
        "audio_filenames = set(audio_predictions['File_Name'])\n",
        "\n",
        "# Find mismatches\n",
        "only_in_text = text_filenames - audio_filenames\n",
        "only_in_audio = audio_filenames - text_filenames\n",
        "\n",
        "# Print differences\n",
        "print(f\"Filenames only in text predictions: {only_in_text}\")\n",
        "print(f\"Filenames only in audio predictions: {only_in_audio}\")\n",
        "\n",
        "# Keep only common filenames\n",
        "common_filenames = text_filenames & audio_filenames\n",
        "\n",
        "# Filter both DataFrames to keep only common filenames\n",
        "text_predictions = text_predictions[text_predictions['File_Name'].isin(common_filenames)]\n",
        "audio_predictions = audio_predictions[audio_predictions['File_Name'].isin(common_filenames)]\n",
        "\n",
        "# Sort and reset index to align both files properly\n",
        "text_predictions = text_predictions.sort_values(by='File_Name').reset_index(drop=True)\n",
        "audio_predictions = audio_predictions.sort_values(by='File_Name').reset_index(drop=True)\n",
        "\n",
        "# Majority fusion without weighting\n",
        "fused_predictions = []\n",
        "for _, (text_row, audio_row) in enumerate(zip(text_predictions.itertuples(), audio_predictions.itertuples())):\n",
        "    File_Name = text_row.File_Name\n",
        "    text_label = text_row.fused_label\n",
        "    audio_label = audio_row.fused_label\n",
        "\n",
        "    # Equal voting\n",
        "    vote_counter = Counter([text_label, audio_label])\n",
        "\n",
        "    # Select the label with the highest count\n",
        "    fused_label = vote_counter.most_common(1)[0][0]\n",
        "    fused_predictions.append({\"File_Name\": File_Name, \"fused_label\": fused_label})\n",
        "\n",
        "# Save to TSV file\n",
        "fused_predictions_df = pd.DataFrame(fused_predictions)\n",
        "output_fused_file = \"/content/SSNCSE_Telugu_Run2.tsv\"\n",
        "fused_predictions_df.to_csv(output_fused_file, sep=\"\\t\", index=False)\n",
        "\n",
        "print(f\"Final fused predictions saved to {output_fused_file}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4R55RUlx-iqp",
        "outputId": "f4f775f2-ba93-4a02-f9d2-04845e4ad1c7"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filenames only in text predictions: set()\n",
            "Filenames only in audio predictions: set()\n",
            "Final fused predictions saved to /content/SSNCSE_Telugu_Run2.tsv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Load the true labels from the Excel file (Sheet1)\n",
        "true_labels_file = \"/content/TE-AT-test.xlsx - Sheet1.tsv\"\n",
        "true_labels_data = pd.read_csv(true_labels_file, sep='\\t')\n",
        "\n",
        "# Load the predicted labels from the TSV file\n",
        "predicted_labels_file = \"/content/SSNCSE_Telugu_Run2.tsv\"\n",
        "predicted_data = pd.read_csv(predicted_labels_file, sep='\\t')\n",
        "\n",
        "# Ensure both datasets have the same 'File Name' column for alignment\n",
        "# Remove leading/trailing spaces from all column names\n",
        "true_labels_data.columns = true_labels_data.columns.str.strip()\n",
        "\n",
        "# Now merge the datasets using the cleaned column names\n",
        "merged_data = pd.merge(true_labels_data[['File_Name', 'Class Label']],\n",
        "                       predicted_data[['File_Name', 'fused_label']],\n",
        "                       on='File_Name')\n",
        "\n",
        "# Extract true labels and predicted labels\n",
        "y_true = merged_data['Class Label']\n",
        "y_pred = merged_data['fused_label']\n",
        "\n",
        "# Continue with the rest of your code...\n",
        "# Encode the true and predicted labels (if needed)\n",
        "label_encoder = LabelEncoder()\n",
        "y_true_encoded = label_encoder.fit_transform(y_true)\n",
        "y_pred_encoded = label_encoder.transform(y_pred)\n",
        "\n",
        "# Generate the classification report\n",
        "print(classification_report(y_true_encoded, y_pred_encoded, target_names=label_encoder.classes_))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLnRO60D-qgr",
        "outputId": "bec2eb5a-3c4d-4fc2-d214-6c9ccc90f7df"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           C       0.86      0.60      0.71        10\n",
            "           G       0.00      0.00      0.00        10\n",
            "           N       0.32      0.80      0.46        10\n",
            "           P       0.00      0.00      0.00        10\n",
            "           R       0.62      0.50      0.56        10\n",
            "\n",
            "    accuracy                           0.38        50\n",
            "   macro avg       0.36      0.38      0.34        50\n",
            "weighted avg       0.36      0.38      0.34        50\n",
            "\n"
          ]
        }
      ]
    }
  ]
}